---
title: "Understanding Google Play Application Reviews"
author: "RTime2Shine"
date: "12/7/19"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-packages, warning = FALSE, message=FALSE}
library(tidyverse)
library(broom)
library(knitr) 
library(skimr)
library(readr)
library(ggplot2)
library(stringr)
library(lubridate)
library(Sleuth3) 
library(ISLR)
library(leaps)
library(dplyr)
library(rms)
```

## Section 1: Introduction (includes introduction and exploratory data analysis)

### Motivaton

As technology has become increasingly prevalent around the world, there has been a change in the consumption of media. One of these ways is via the purchase of applications (apps) for various smartphones and other devices. Several technology companies, including Apple and Google, run virtual stores for these apps in which a person can download an app for their device. These apps can be for various purposes like socializing, playing games, or watching television and movies, among others. While any user of a phone can agree that apps hold an important effect on how one interacts with technology on a daily basis, the weight of the impact becomes even more shocking when one looks at the figures- in 2018, global app downloads topped 194 billion (Dignan).

Our motivation for this project is to understand what makes apps (from the Google Play store specifically) have favorable ratings. Understanding the ratings of an app is important for several reasons. First, ratings can be important to the provider- in this case Google- who can decide whether an app should continue to be sold to maintain their quality standards. Ratings are additionally useful as a direct line of communication between the user and the developers- often, developers are made aware of changes that need to be made to their apps through user feedback. Lastly, reviews serve to inform potential users of an app whether or not it is worth their time and can affect future downloads. Considering that app users are predicted to spend about $120 billion in app stores in 2019, understanding which apps do well on the Play Store and what factors affect app performance is an immensely important question to gain more insight into. 

### Research Question & Hypothesis

Our ultimate goal is to create a model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. This will enable us to see which predictor variables interact with each other to effect the rating for a given app. We posit that examining such relationships will help developers understand what factors may influence an app's rating and use that information to create better applications for consumers. As well, conglomerates such as Google (whom this dataset is from) can use this information to more accurately display or promote apps that meet these characteristics or promote ads related to these apps and generate revenue. 

This leads us to introduce our main research question: What are the relevant factors that affect the rating given for apps in the Google Play store? Although this project will give a detailed attempt to answer this question, our preliminary hypothesis is that the variables Category, Price, Installs, and Content Rating are the predictor variables that will most affect a given app rating and popularity, as measured by the number of installs of the app. We believe that these variables are indicative of an apps useability and likeness (as determined by variables like content rating and categories in how people may be drawn to an app) as well as its accessibility (price). Furthermore, once we test our hypothesis and determine which factors are relevant, we will attempt to use that information to predict the success of an app as measured by its rating.

### The Data

The dataset was obtained from Kaggle. According to Kaggle, the dataset was scraped directly from the Google Play Store in August 2018. Each observation represents one individual app on the Google Play Store. This particular dataset has 13 variables with 10841 observations. The variables consist of various information collected about each application (which represents a row) in the dataset. This information includes the apps category in the app store, its average rating, price, content rating, the number of installs, among other metrics. In our Exploratory Data Analysis, we will further explain the use of each of these variables and determine which of these may be significant for and relevant in our analysis. The response variable in our investigation is `Rating` which is the mean rating out of 5.0 for an application in the Google Play Store. This is a numeric variable. 

### Exploratory Data Analysis

```{r load-data, warning = FALSE, message=FALSE}
apps <- read_csv("/cloud/project/02-data/googleplaystore.csv")
skim(apps)
levels(apps$`Content Rating`) <- c("Adults Only","Everyone","Mature", "Teen", "Unrated")
apps <- filter(apps, Reviews <= 203579)
apps$`Rating` <- as.numeric(apps$`Rating`)
apps <- apps %>%
  na.omit(apps)
```

```{r fact recode, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(Category = as.factor(Category)) %>%
  mutate(Size = as.factor(Size)) %>%
  mutate(Installs = as.factor(Installs)) %>%
  mutate(Type = as.factor(Type)) %>%
  mutate(`Content Rating` = as.factor(`Content Rating`)) %>%
  mutate(Genres = as.factor(Genres)) %>%
  mutate(Category = as.factor(Category))

apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
apps$Price <- as.numeric(gsub('[$.]', '', apps$Price))
apps$`Android Ver`<-gsub("\\..*","",apps$`Android Ver`)
apps$`Android Ver`<-as.factor(apps$`Android Ver`)
apps$`Android Ver`[apps$`Android Ver` == "NaN"] <- NA
```

#### Data Wrangling

Upon examining our predictor variables, it looks like there are occasionally one or two observations missing in the dataset, which does not raise lots of concern. It is worth noting, however, that 1474 of our response variable values are missing. This is roughly 10% of the data. Given that the data was web scraped, we will assume that the reason behind these missing values is that there was not a mean rating value for those particular observations (app). Thus, we will omit all of the NA values and continue to investigate only those apps for which we have ratings. 

Furthermore, here it is worth noting that the variable, `Genres` contains the same information in the `category` variable - the only difference being that the data is just displayed a bit differently. Therefore, as to avoid being redundant, we will only be using `category` in our analysis. As well, the information contained in the variable `Type` is also contained in `Price`. Since `Price` provides the prices of an app and `Type` simply denotes wether or not an app is paid or free. There is mostly likely a large linear dependency between these two variables as they measure the same thing, so we will use `Price` instead of `Type.` We will not deleting theese variables from the dataset as to maintain integrity, but will not examine them in our analysis.

As well, we also have many of predictors that are coded as characters in the dataset, so we decided to recode them as factors. We also have some variables that are coded as characters due to the existence of a particular a symbol (ex. $), we will recoded these into a format which will be usable for our analysis. 

Looking at the data, there are two variables related to the version, or iteration of the app as provided by the developers: `Current Ver` and `Android Ver`. Given that Google owns both Android and the Google Play Store, the company would likely be more interested in the Android version of the app. Furthermore, Android users are unlikely to be using other operating system's application stores, so a developer who is interested in creating apps for the Android market would gain more information through an examination of the compatibility of certain apps with a particular version of Android. Some data wrangling was necessary to make this variable suitable for analysis.

We also created a variable called `date_since`, which marks the number of days that the app has been updated since the day that the data was scraped on August 8, 2018. This will allow us to determine how recent the last update was for a particular app and provides some information related to the relative frequency of updates and how that may affect an app's rating.

```{r dates, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(date_new = mdy(`Last Updated`))
apps <- apps %>%
  mutate(date_since = mdy("August 8, 2018") - date_new)
```

There are also a number of variables which required releveling. Many of our predictors have multiple levels and these may have made our model too complicated. To reduce the probabiltiy that the model overfits the data and is too complicated, we will relevel our variables as follows:

Since our variable `Price` is currently not numeric and isn't coded into categories, we will relevel price into 3 categories: Free, Between 0 and 4.99 dollars, and greater than 5 dollars. 

```{r price-relevel, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(Price = case_when(Price == 0 ~ "Free", Price < 500 & Price > 0 ~ "Between $0 and $4.99", Price > 500 ~ "Greater than $5"))

apps <- apps %>%
  mutate(Price = fct_relevel(Price, "Free", "Between $0 and $4.99", "Greater than $5"))
```

Since our variable `Size` is currently very widely distributed, we will relevel size into 3 categories: Varies with Device, Less than 100, and Greater than 100.

```{r sizerelevel, warning = FALSE, message = FALSE}
apps$`Size`<- sub('k$', '', apps$`Size`)
apps$`Size`<- as.numeric(gsub("Varies with device", 0.001 , apps$`Size`))
apps <- apps %>%
  mutate(Size = case_when(Size == 0.001 ~ "Varies with device", Size < 100 ~ "Less than 100 MB", Size > 100 ~ "Greater than 100 MB"))
```

Since our variable `Installs` is currently very widely distributed, we will relevel installs into 3 categories: Less than 100, Between 100 and 1,000, Between 1,000 and 10,000, Between 10,000 and 100,000, and 100,000 or Greater.

```{r installs-relevel, warning = FALSE, message = FALSE}
apps$`Installs`<- as.numeric(gsub(",", "", apps$`Installs`))

apps <- apps %>%
  mutate(Installs = case_when(Installs < 100 ~ "Less than 100", Installs < 1000 & Installs >= 100 ~ "Between 100 and 1,000", Installs < 10000 & Installs >= 1000 ~ "Between 1,000 and 10,000", Installs < 100000 & Installs >= 10000 ~ "Between 10,000 and 100,000", Installs >= 100000 ~ "100,000 or Greater"))


apps <- apps %>%
  mutate(Installs = fct_relevel(Installs, "Less than 100", "Between 100 and 1,000", "Between 1,000 and 10,000", "Between 10,000 and 100,000", "100,000 or Greater"))

```

Our variable `Category` is extremely large and has many levels as shown in the graphiic below. To simplify this, we will create a new variable called `category_simp` with two levels: one for the top 6 categories ("FAMILY","GAME", "TOOLS","MEDICAL", "LIFESTYLE", and "FINANCE") and another for all the other categories. 

```{r category pop, warning = FALSE, message = FALSE}
apps %>%
  count(Category) %>%
  arrange(-n) %>%
  mutate(freq = n/sum(n))
```

```{r relevel cat, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(category_simp = case_when( 
    apps$Category %in% c("FAMILY","GAME", "TOOLS","MEDICAL", "LIFESTYLE", "FINANCE") ~ "Top 6 Categories", 
    TRUE ~ "Others"))

apps %>%
  count(Category, category_simp) %>%
  arrange(desc(n))
```

As shown in the above table, we can see that the top 6 categories are in one level and the others are stored in another level. 

```{r relevel-andriod-ver, warning = FALSE, message=FALSE}
apps %>%
  count(`Android Ver`) %>%
  arrange(desc(n))

apps <- apps %>%
  na.omit(`Android Ver`) %>%
  mutate(androidver_simp = case_when( 
    `Android Ver` %in% c(1,2,3,4) ~ "1-4", 
    `Android Ver` %in% c(5,6,7,8) ~ "5-8",
    TRUE ~ "Varies with Device"))

apps %>%
  count(`Android Ver`, androidver_simp) %>%
  arrange(desc(n))
```

As well, the variable `Android Ver` is quite complicated. To simplify our analysis we decided to bin this variable into two levels, one for apps that support the first "generation" of Android, versions 1-4, and another for apps that support the second generation, 5-8. For apps that vary with device, we kept those observations as is. As shown in the final graphic, our new variable, `androidver_simp` reduces some of the complexity in the previous variable while retaining ease of use for analysis. 

```{r relevel-content, warning = FALSE, message=FALSE}
apps <- apps %>%
  mutate(content_simp = case_when( 
    `Content Rating` == "Everyone" ~ "Everyone",
    `Content Rating` == "Teen" ~ "Teen",
    `Content Rating` == "Mature " ~ "Mature",
    `Content Rating` == "Everyone " ~ "Everyone",
    `Content Rating` == "Adults only " ~ "Adults only",
    `Content Rating` == "Unrated" ~ "Unrated"))

apps %>%
  count(`Content Rating`, content_simp) %>%
  arrange(desc(n))
```

Lastly, there were also issues with our variable `Content Rating`. This particular variable had two categories for the rating "Everyone," one of which happend to be coded with a space ("Everyone ") in the dataset. Knowing that content ratings are finite, we decided to combine these categories together as to reduce the number of levels in this variable and its redundancy. 

#### Univariate Analysis

##### Ratings

```{r rating-distribution, warning = FALSE, message = FALSE}
ggplot(data = apps, aes(x = Rating)) + 
  geom_histogram(binwidth = 0.1, fill = "blue") + 
  xlim(0,5) + labs(title = "Distribution of App Ratings")
```

```{r summary-stats, warning = FALSE, message = FALSE}
apps %>% 
  summarise(median(Rating), IQR(Rating))
```

As seen above, the distribution of ratings is left-skewed with its peak between 4 and 4.5. The IQR of 0.6 demonstrates that the middle 50% of our data is highly concentrated around that peak. 

##### Reviews

```{r Reviews, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Reviews)) +
  geom_histogram() +
  labs(title = "Distribution of Reviews")

apps %>%
  summarise(median(Reviews)) 
apps %>%
  summarise(max(Reviews))
```

```{r log-reviews-1, warning = FALSE, message = FALSE}
apps <- apps %>% 
  mutate(log_reviews = log(Reviews)) 
ggplot(apps, aes(x = log_reviews)) + geom_histogram(binwidth = 0.5) + labs(title = "Distribution of Log Transformed Reviews")
apps %>%
  summarise(median(log_reviews), max(log_reviews), IQR(log_reviews))
```

Reviews is definetley one of our most skewed predictors, as shown by the strong right skew in this histogram. Originally, the skew was even more extreme, since there was one outlier that had 78 million reviews. Therefore, to correct this issue, we removed the outlier from the variable as well as log transformed it in order to make it easier to see trends as well as make the variable less skewed.

##### Category

```{r category, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
apps %>%
    count(Category) %>%
    mutate(Category = fct_reorder(Category, n, .desc = FALSE)) %>%
    ggplot(aes(x = Category, y = n)) + geom_bar(stat = 'identity') + coord_flip() + labs(title = "Frequency of Category", y = "Count")
```

```{r log reviews, warning = FALSE, message = FALSE}
ggplot(apps, aes(x = category_simp)) + geom_bar() + labs(title = "Frequency of Category", subtitle = "Simplified Version")
```

As demonstrated above, there are a number of levels within the variable `Category`, this will cause problems with overfitting down the line, so we decided to create two levels- one for the most popular levels, and one with the least popular levels. Once binned into those two levels, there is a much more even spread. 

##### Size

```{r size, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Size)) +
  geom_histogram(stat="count", binwidth = 0.00025) +
  labs(title = "Distribution of App Sizes")
```
 
 The above plot demonstrates that the most common Size type is "Less than 100 MB" with more than 6000 of the applications being of that Size. 
 
##### Installs

```{r installs, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Installs)) +
  geom_histogram(stat="count") +
  coord_flip()+
  labs(title = "Distribution of Installs")
```

Given the above plot, the most common number of installs within the dataset is 100,000 or Greater. 

##### Price

```{r price, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Price)) +
  geom_histogram(stat="count")+
  coord_flip() +
  labs(title = "Distribution of Price (in USD)")
```

Given the above plot, the most common Price for the applications whithin the dataset is "Free". For our analysis, we decided to use the variable `Price` because it has more levels and is a more specific classification of the price of the applications than `Type` which just indicated free or paid. 


##### Content Rating

```{r cr, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = content_simp)) +
  geom_histogram(stat="count") +
  labs(title = "Distribution of Content Rating")
```

Given the above plot, the most common content rating is "Everyone" within the dataset. 

##### Android Version 

```{r and-plot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = androidver_simp)) +
  geom_histogram(stat="count") +
  labs(title = "Distribution of Android Version", x = "Android Version")
```

Given the above plot, within the dataset, the most common Android Version is 1-5. 

```{r datesince-plot, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = date_since)) +
  geom_histogram(stat= "count") +
  labs(title = "Frequency of Date Since Last Updated", subtitle = "Based off of number of days since August 1, 2018")
apps %>%
  summarise(median(date_since), max(date_since), IQR(date_since))
```

Given the above plot, for 50% of the observations it has been greater than 92 days since the last update and for the other 50% of the observations, it has been less than 92 days since the last update. 

#### Bivariate Analysis

A bivariate analysis between variables will help understand the interaction between indidivual predictor variables and the response. 

```{r category-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = category_simp, y = Rating)) + geom_boxplot() + coord_flip() +
  labs(title = "Relationship between Category and Rating") 
```

Although there is some variation in rating between app categories, the most telling aspect of this exploratory model is the outliers. It appears that some categories are more susceptible to outliers with low ratings. More over there are notable discrepancies between minimum boxplot rating among categories. **NOTE THIS NEEDS TO BE FIXED this was copy pasted from the old one i think

```{r reviews-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = log_reviews, y = Rating)) + geom_point() +
  labs(title = "Relationship between Log Reviews and Rating", x = "Number of Reviews") 
```

Based on the scatterplot above, there is likely a relationship between number of reviews and app rating. As the number of reviews increased the app rating was concentrated at approximately 4.5 - which was consistent with apps holding smaller number of reviews. IS THIS RIGHT??

```{r size-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = Size, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Size and Rating", x = "Size")
```

```{r installs-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = Installs, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Installs and Rating", x = "Number of Installs") + coord_flip()
```

The boxplot above clearly shows a significant relationship between number of installs and rating. As the number of installs increases the IQR appears to decrease in conjunction. Moreover median rating also increases with number of installs. 

```{r type-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = Price, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Price and Rating")
```

```{r content-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = content_simp, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Content Rating and App Rating")
```

```{r andsimp-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = androidver_simp, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Android Version and Rating")
```

```{r datesince-rating, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = date_since, y = Rating)) + geom_point() +
  labs(title = "Relationship between Days Since Last Updated and Rating", x = "Days Since") 
```

## Section 2: Regression Analysis (includes the final model and discussion of assumptions)

### Model Process

The regression modeling technique we will use will be Multiple Linear Regression (MLR). Since we are exploring the effect of multiple predictor variables on our response, `rating`, it is apt that we use MLR to model our data. MLR allows us to see the effect of multiple predictors on a response and explore both the significance of each predictor on the response as well as the effect of each predictor on the response. As opposed to Simple Linear Regression, MLR allows us to measure the effect of multiple predictors on your response in one model - SLR only allows us to measure the effect of one predictor on the response in one model. This is very taxing and inefficient for the number of predictors we want to measure. As well, there may be interactions between these predictors that we will be unable to view using SLR. MLR allows us to both model and view the amalgamation of these predictors in their effects on the response variable. MLR from both an efficiency and relevancy perspective is much better suited to model our data as opposed to other methods. 

Our ultimate goal is to create the model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. We will attempt to choose a model using a minimization of both BIC and AIC as our criteria as this will allow us to calculate a precise prediction of our response variable while also removing extraneous predictors. We will use BIC and AIC as our selection criteria as it penalizes more for erroneous predictors as compared to adj. R-Squared. We will not use R-squared as a criteria for model selection. R squared increases strictly as the number of predictors increases and does not tell us if these additional predictors are significant or not. If we used r-squared we would always choose models with the largest numbers of predictors, which would not always produce the simplest, most accurate model. Unlike R squared, AIC, BIC, and adjusted R squared do penalize for insignificant predictors and can give us a better idea of which predictors actually contribute to the response variable. 

In order to find our final model, we will use a process of both forwards and backwards selection slowly adding a combination of relevant predictors into our model. We will then check the BIC and AIC values for each of these models and find the model with the lowest value overall, or the fewest predictors - this will be the model that most accurately predicts our response with the fewest number of predictors. We will then plot each predictor on the response to determine if the effect is relevant or if there are possible interactions between other variables. As well, we will need to consider potential outliers and extraneous values in our model. Using the distributions of standardized residuals and a calculation of Cook’s distance, we will attempt to determine those observations with high standardized residuals or cook’s distance and determine if those observations have a significant effect on our model. Lastly, we will need to find the VIF factor for each of our final predictors to see if there is any collinearity between them. A VIF greater than 10 would require us to explore possible ways to mitigate interactions between variables or consider dropping predictors are are too heavily correlated. 

### Model Selection

```{r full-model, warning = FALSE, message=FALSE}
apps <- apps %>%
  na.omit(apps)
full_model <- lm(Rating ~ category_simp + log_reviews + Size + Installs + Price + content_simp + androidver_simp + date_since, data = apps)
kable(tidy(full_model),format="html",digits=4)
```

Above is the output of our full model using the predictors we deemed to be relevant in our Exploratory Data Analysis. As shown, there are a number of predictors that appear to have p-values greater than 0.05, deeming them insiginifcant. To find the most parsimonious model for our purporses of prediction, we used a model selection process comparing both BIC and AIC using a combination of backwards, forwards, and hybrid selection. The final model we chose was that selected using both AIC forward and hybrid selection as this particular model (as displayed below) gave us the shortest model while retaining assumptions better than the other models we tried. After going through the model process, we found that the constant variance assumption was violated using our BIC model. As well, it had far too many levels and we afraid that it was "overfitting" our data. Our model determines its coefficients through the relative variance of different variables in our data. If there are some predictors which have high frequency in the data, it is most likely that those predictors will account for the highest amount of variance when developing our final model. Thus, our model would tend to "memorize" our data rather than rather assessing the variability in response through particular trends in our predictors generalizable to the app store as a whole - as opposed to this singular dataset. In our additional work section, we have attached the final versions of those models as well as a plot of the residuals to illustrate this violation. 

```{r aic-forward, warning = FALSE, message=FALSE}
apps <- apps %>%
  na.omit(apps)
null_model <- lm(Rating ~ 1, data = apps,)
regfit_forward <- step(null_model, scope = formula(full_model), 
                       direction = "forward")
```


```{r final model, warning = FALSE, message=FALSE}
final_model <- lm(Rating ~ category_simp + Size, data = apps)
kable(tidy(final_model),format="html",digits=3)
```


As shown above, our final model is as follows:

`Rating`^ = 4.00969938 - 0.0268642 * `category_simpTop 6 Categories`+ 0.1523095 * `SizeLess than 100 MB` + 0.1856302 * `SizeVaries with device`

As well, it can be seen that all of the predictors in our final model have p-values less than 0.05, which allows us to conclude that they are all statistically significant - indicating that a lot of the terms in the model are making a significant contribution to understanding the variation in the response, `Rating`. 

### Interactions & Our Updated Model

Now, we will conduct an f-test including the possible interaction effects in our model to determine if there are any interaction effects which are significant.

Our hypotheses for this F-test is as follows:

Since the number of levels of our categorical variables are k > 2 we can use a nested F test to determine if any of these interaction effects are significant. 

Ho: B(1) = B(2) = ... = 0

Ha: Atleast one Beta_j is not equal to 0

In this case, we are measuring the interaction between `category_simp` and `Size`, so our Beta j's denotate the coefficient for both of these predictors as well as the interaction term. Our null hypothesis can essentially be read as there are no interactions between any of the variables in our model and our alternative can be read as that there is a significant interaction effect in the model since there are only two predictors in our final model. 

```{r testing interactions, warning = FALSE, message=FALSE}
reduced <- final_model
full  <- lm(Rating ~ category_simp + Size + category_simp*Size, data = apps)
kable(anova(reduced, full), format="markdown", digits = 3)
```

Since our p-value of approximately 0.5727 is greater than our significance level of 0.05, we do not have evidence that there is a statistically significant interaction between both `Size` and `category_simp`. Thus, our final model will not include this interaction term and we will proceed with our previously defined final model. 

### Assumptions

#### Linearity

The Linearity Assumptions assumes that the response variable has a linear relationship with the predictor variables used in the final model. To assess linearity, we look at the plots created in the Exploratory Data Analysis. None of these plots seem to have a non-linear relationship such as one that would be polynomial; however, some are quite skewed but this should not matter for the purposes of our model. To illustrate this, we will re-display the bivariate plots with our final predictor variables. 

```{r linearity, warning = FALSE, message=FALSE}
ggplot(data = apps, aes(x = category_simp, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Category and Rating")

ggplot(data = apps, aes(x = Size, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Size and Rating")
```

As shown in the plots above, both the predictor variables appear to satisfy the linearity assumption.

#### Constant Variance

The Constant Variance Assumption assumes that the regression variance is the same for all of the predictor variables in the model. To test this assumption, we will plot the residual values against predictors.

```{r resid-plots, warning = FALSE, message=FALSE}
apps <- apps %>% 
  mutate(predicted = predict.lm(final_model), residuals = resid(final_model)) #mutates residuals to the model
ggplot(data=apps,aes(x=predicted, y=residuals)) + #plots residuals vs predictors
  geom_boxplot() + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Predicted Values")
```

As shown, it appears that the constant variance assumption is violated. There appears to be a clear pattern in our residuals relating to the significance of large amounts of negative values. Since our response was normally distributed at the beginning of our EDA, we did not see the need for a transformation of our response; however, it may be the case that there is a need for more, higher-order interaction terms. Since our model satisfies the linearity assumption and also realtively satisfies our normality assumption it is reasonable to assume that this is not a great cause for concern. As well, as stated above and in our additional work we did try to choose multiple models and this particular one was the one with the least noticeable violation of constant variance and normality. Therefore, we believe that given this sufficient effort, this model is mostly likely the one that is grestest for predicting the majority of applications given our dataset. 

#### Normality

The Normality Assumption assumes that for any given set of predictors, the response variable, `ratings`, follows a Normal distribution around its mean. To test this, we will make a Normal QQ plot. 

```{r norm-qq, warning = FALSE, message=FALSE}
 ggplot(data = apps, mapping = aes(sample = residuals)) + 
   stat_qq() + 
   stat_qq_line() +
   labs(title = "Normal QQ Plot of Residuals")
 
 ggplot(data = apps, mapping = aes(x = residuals)) + 
   geom_histogram()+
   labs(title = "Distribution of Residuals")
```

As shown, our residuals are relatively normally distributed in our histogram. The center is around 0 which is good; however there is a slight left skew. As well, our Normal-QQ plot doesn't closely match the idea line at the beginning of the more negative values. It may be the case that there are many outliers or high leverage points in our model that are causing this skew and if they were removed then we wouldn't see this. However, since our distribution of residuals is relatively normal and the Normal QQ Plot is mostly following the trend line it is reasonable to assume that this assumption is satisfied due to the robustness of our model. In our model Assessment, we can further fix these issues and improve our model. 

#### Independence

The Independence Assumption assumes that all observations in the data used to construct the model are independent of each other. Given that each observation and is not dependent on the time frame or location of collection for its mean rating. The observations are independent of each other and thus the Independence Assumption is maintained. 

### Model Assesment

#### Leverage

According to lecture, the threshold we should use for determining if observations are high leverage points is: hi > (2(p+1))/n

```{r leverage, warning = FALSE, message=FALSE}
apps_output <- augment(final_model) %>%
  mutate(obs_num = row_number())

leverage_threshold <- (2*(12+1))/nrow(apps)
ggplot(data = apps_output, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.3) + 
  geom_hline(yintercept = leverage_threshold,color = "red")+
  labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
  geom_text(aes(label=ifelse(.hat > leverage_threshold, as.character(obs_num), "")), nudge_x = 4)
```

Let’s filter to see the total # of points that crossed the threshold.

```{r filterlev, warning = FALSE, message=FALSE}
apps_output %>% filter(.hat > leverage_threshold) %>%
  select(category_simp, Size)
```

As shown, there are 214 high leverage points. This is problematic but before we assess our model's strength we should check cook's distance. However, in our previous model we had 8,134 leverage points. Thus, this model is a significant improvement over that. 


#### Cook's Distance

Now let’s check how many of these points cross our threshold for Cook’s Distance (Di > 1) to determine if these high leverage points do in fact have a significant influence on our model coefficients.

```{r cooks-distance, warning = FALSE, message=FALSE}
ggplot(data = apps_output, aes(x = obs_num, y = .cooksd)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=1,color = "red")+
  labs(x= "Observation Number",y = "Cook's Distance",title = "Cook's Distance") +
  geom_text(aes(label = ifelse(.hat>1,as.character(obs_num),"")))
```

Let's check to see the number of points that violated this threshold. 

```{r filtercook, warning = FALSE, message=FALSE}
apps_output %>% filter(.cooksd > 1) %>%
  select(category_simp, Size)
```

As shown, none of these violate Cook's Distance - so it is most likely the case that although there may be some skew or slight violation of assumptions, our model still is relatively strong in predicting Rating and that there isn't a cause for concern. It is unlikley that any of these high leverage points have a significant influence on the model coefficients.

#### Standardized Residuals
Now, let’s plot our standardized residuals to see if there are any points which break the threshold |resid std.| > 2.

```{r standardresid-predicted, warning = FALSE, message=FALSE}
ggplot(data = apps_output, aes(x = .fitted,y = .std.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = 0,color = "red") +
  geom_hline(yintercept = -2,color = "red",linetype = "dotted") +
  geom_hline(yintercept = 2,color = "red",linetype = "dotted") +
  labs(x ="Predicted Value",y ="Standardized Residuals",title = "Standardized Residuals vs. Predicted") +
  geom_text(aes(label = ifelse(abs(.std.resid) >2,as.character(obs_num),"")), nudge_x = 0.08)
```

As depicted the plot, there are a number of points which violate this threshold, so let’s filter the data to get an exact number.

```{r filtersd, warning = FALSE, message=FALSE}
apps_output %>% filter(abs(.std.resid) > 2) %>%
  select(category_simp, Size)
```

As illustrated, there are 363 observations with a standardized residual greater than +/- 2. These observations are considered to have standardized residuals with large magnitude. This us much better than our previous model which had greater than 470 points that violated this threshold

```{r residgram, warning = FALSE, message=FALSE}
ggplot(data = apps_output, aes(x = .std.resid)) +
  geom_vline(xintercept = 2,color = "red") +
  geom_vline(xintercept = -2,color = "red") +
  geom_histogram() +
  labs(x = "Residuals")
```

```{r percent, warning = FALSE, message=FALSE}
percentage <- (363/nrow(apps)) * 100
print(percentage)
```

The proportion of observations that have standardized residuals with magnitude > 2 is about 0.4702682, or 4.702% of the observations. Since this proportion is less than 5%, it is most likely the case that it is not statistically significant and is relatively small. Therefore, there is most likely not a concern with the number of observations flagged as having standardized residuals with large magnitude as the proportion of these residuals is relatively small. Although our proportion is somewhat close to 5% it is still most likely the case that these flagged residuals are a relatively small proportion of the data, as well since none of our points violated Cook's Distance, it is most likely the case that our model is sound. As well, it is most likely these 5% of observations which may have caused the skew in our residual variance. 

### VIF

We will check the VIF of our model without interactions:
```{r vif, warning = FALSE, message=FALSE}
tidy(vif(final_model))
```

None of our variables have VIF > 10, indicating that there were no concerns with multicollinearity in our final model and there was no need for an interaction, confirming the results of our F-test. 

### Model Interpretation

Our model allows us to make various predictions about what impacts an application’s mean rating. We see that the predictor variable with the most influence on the response- if the size of an application is less than 100 MB the mean rating is expected to increase by 0.1523095 and if the size varies with device, the mean rating is expected to increase by 0.1856302. The other variable deemed to be significant is cateogrry_simpTop 6 Categories meaning that if an application is in the top 6 categories, the mean rating is expectected to decrease by 0.0268642. Thus, for an application developer, the size of the application and the category of the applications will be a factor that they would want to focus on if they are attempting to alter the mean rating of their application.

Another thing worth noting is that the intercept is quite high. At 4.0096938, it is stating that for an application that is not within the top 6 categories and is 100 MB or larger the mean rating is estimated to be 4.0096938, which is quite close to the highest possible rating, 5.


## Section 3: Discussion and Limitations 

With regards to the data we used, it is worth noting that the data we used was web scrapped all at one point by a third party. Therefore, due to time and human error, there is bound to be false or missing data. Another thing to note is that several of our variables, such as reviews and content rating, were incredibly skewed. On of the more major issues with this dataset was that 1474 of the 10841 observations were missing a response. That means that 10% of our observations were missing throughout the modelling process potentially creating bias and skew.

If we were to start over with this project, we would probably look more closely at the dataset. Many of our variables had issues that needed to be taken care of in order to conduct multilinear regression. For instance, we needed to completely relevel a categorical variable, Category. There were other variables that were challenging such as a time related variable and variables related to the version of the applications. If we were to restart the project, we would probably find a different dataset. 

If we were to continue the current project with the current dataset we would have explored how the factors contributing to app success change over time and specifically from year-to-year, we would set different baseline levels for every combination of categorical variables and redo the models and model selection. To improve our analysis, we would make training and test cases and conduct a k-folf cross validation that will allow us to assess the model's fit allowing us to make sure that the model is not too closely fit as to check generalizability, but also make sure that it is specific enough to the data. Regarding the data itself, we would probably attempt to webscrape the data now as to have data that more closely reflects the current situation. 

## Section 4: Conclusion

In conclusion, our project’s goal was to understand what factors contribute to an apps success as measured by the average rating given to an application. By conducting multinormial regression analysis, we found that the categories and size variables are vital to the outcome of the mean ratings for an application. This can be useful to app developers, users, and application stores in how to view, change, and ustiliza applications. 

## Section 5: Additional Work

### Model Selection

```{r bic-back, warning = FALSE, message=FALSE}
regfit_backward <- regsubsets(Rating ~ category_simp + log_reviews + Size + Installs + Price + content_simp + androidver_simp + date_since, data = apps, method="backward")
sel_summary <- summary(regfit_backward)
coef(regfit_backward, which.min(sel_summary$bic))
```

```{r bic-forward, warning = FALSE, message=FALSE}
regfit_forward <- regsubsets(Rating ~ category_simp + log_reviews + Size + Installs + Price + content_simp + androidver_simp + date_since, data = apps, method="forward")
sel_summary <- summary(regfit_forward)
coef(regfit_forward, which.min(sel_summary$bic))
```

Above is our model selection using BIC, we chose not to include this particular model for multiple reasons, including the fact that the constant variance assumption of this model is violated. 

```{r aic-back, warning = FALSE, message=FALSE}
apps <- apps %>%
  na.omit(apps)
null_model <- lm(Rating ~ 1, data = apps,)
regfit_backward <- step(full_model, direction = "backward")
```

```{r hybrid, warning = FALSE, message=FALSE}
regfit_hybrid <- step(null_model, scope = formula(full_model), 
                      direction = "both")
```

Above is the model selection for both Hybrid and Backward AIC selection. Our final model was the one that was produced with both Hybrid and forward selection.

Here is the residual plot for the model produced by BIC selection:

```{r final-model2, warning = FALSE, message=FALSE}
old_model <- lm(Rating ~ log_reviews + Installs + Price + content_simp, data = apps,)
apps <- apps %>% 
  mutate(predicted = predict.lm(old_model), residuals = resid(old_model)) #mutates residuals to the model
ggplot(data=apps,aes(x=predicted, y=residuals)) + #plots residuals vs predictors
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Predicted Values")
```

As shown there is a sigificantly visible pattern in this model's residuals, far worse than our current model. 

### References

Gupta, Lavanya. Kaggle. Jan. 2019, www.kaggle.com/lava18/google-play-store-apps?fbclid=IwAR36EMS2jg5fhPi-BQlX6Mv4MCk8YUm2XmyOLt0zsKkNyc9JK-JD7aLy-6I. Accessed 30 Oct. 2019. 

Dignan, Larry. “App Economy Expected to Be $120 Billion in 2019 as Small Screen Leads Digital Transformation Efforts.” ZDNet, ZDNet, 16 Jan. 2019, www.zdnet.com/article/app-economy-expected-to-be-120-billion-in-2019-as-small-screen-leads-digital-transformation-efforts/
