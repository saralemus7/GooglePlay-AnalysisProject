---
title: "Understanding Google Play Application Reviews"
author: "RTime2Shine"
date: "12/7/19"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-packages, warning = FALSE, message=FALSE}
library(tidyverse)
library(broom)
library(knitr) 
library(skimr)
library(readr)
library(ggplot2)
library(stringr)
library(lubridate)
library(Sleuth3) 
library(ISLR)
library(leaps)
library(dplyr)
library(rms)
```

## Section 1: Introduction (includes introduction and exploratory data analysis)

### Motivaton

As technology has become increasingly prevalent around the world, there has been a change in the consumption of media. One of these ways is via the purchase of applications (apps) for various smartphones and other devices. Several technology companies, including Apple and Google, run virtual stores for these apps in which a person can download an app for their device. These apps can be for various purposes like socializing, playing games, or watching television and movies, among others. While any user of a phone can agree that apps hold an important effect on how one interacts with technology on a daily basis, the weight of the impact becomes even more shocking when one looks at the figures- in 2018, global app downloads topped 194 billion (Dignan).

Our motivation for this project is to understand what makes apps (from the Google Play store specifically) have favorable ratings. Understanding the ratings of an app is important for several reasons. First, ratings can be important to the provider- in this case Google- who can decide whether an app should continue to be sold to maintain their quality standards. Ratings are additionally useful as a direct line of communication between the user and the developers- often, developers are made aware of changes that need to be made to their apps through user feedback. Lastly, reviews serve to inform potential users of an app whether or not it is worth their time and can affect future downloads. Considering that app users are predicted to spend about $120 billion in app stores in 2019, understanding which apps do well on the Play Store and what factors affect app performance is an immensely important question to gain more insight into. 

### Research Question & Hypothesis

Our ultimate goal is to create a model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. This will enable us to see which predictor variables interact with each other to effect the rating for a given app. We posit that examining such relationships will help developers understand what factors may influence an app's rating and use that information to create better applications for consumers. As well, conglomerates such as Google (whom this dataset is from) can use this information to more accurately display or promote apps that meet these characteristics or promote ads related to these apps and generate revenue. 

This leads us to introduce our main research question: What are the relevant factors that affect the rating given for apps in the Google Play store? Although this project will give a detailed attempt to answer this question, our preliminary hypothesis is that the variables Category, Price, Installs, and Content Rating are the predictor variables that will most affect a given app rating and popularity, as measured by the number of installs of the app. We believe that these variables are indicative of an apps useability and likeness (as determined by variables like content rating and categories in how people may be drawn to an app) as well as its accessibility (price). Furthermore, once we test our hypothesis and determine which factors are relevant, we will attempt to use that information to predict the success of an app as measured by its rating.

### The Data

The dataset was obtained from Kaggle. According to Kaggle, the dataset was scraped directly from the Google Play Store in August 2018. Each observation represents one individual app on the Google Play Store. This particular dataset has 13 variables with 10841 observations. The variables consist of various information collected about each application (which represents a row) in the dataset. This information includes the apps category in the app store, its average rating, price, content rating, the number of installs, among other metrics. In our Exploratory Data Analysis, we will further explain the use of each of these variables and determine which of these may be significant for and relevant in our analysis. The response variable in our investigation is `Rating` which is the mean rating out of 5.0 for an application in the Google Play Store. This is a numeric variable. 

### Exploratory Data Analysis

```{r load-data, warning = FALSE, message=FALSE}
apps <- read_csv("/cloud/project/02-data/googleplaystore.csv")
skim(apps)
levels(apps$`Content Rating`) <- c("Adults Only","Everyone","Mature", "Teen", "Unrated")
apps <- filter(apps, Reviews <= 203579)
apps$`Rating` <- as.numeric(apps$`Rating`)
apps <- apps %>%
  na.omit(apps)
```

```{r fact recode, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(Category = as.factor(Category)) %>%
  mutate(Size = as.factor(Size)) %>%
  mutate(Installs = as.factor(Installs)) %>%
  mutate(Type = as.factor(Type)) %>%
  mutate(`Content Rating` = as.factor(`Content Rating`)) %>%
  mutate(Genres = as.factor(Genres)) %>%
  mutate(Category = as.factor(Category))

apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
apps$Price <- as.numeric(gsub('[$.]', '', apps$Price))
apps$`Android Ver`<-gsub("\\..*","",apps$`Android Ver`)
apps$`Android Ver`<-as.factor(apps$`Android Ver`)
apps$`Android Ver`[apps$`Android Ver` == "NaN"] <- NA
```

#### Data Wrangling

Upon examining our predictor variables, it looks like there are occasionally one or two observations missing in the dataset, which does not raise lots of concern. It is worth noting, however, that 1474 of our response variable values are missing. This is roughly 10% of the data. Given that the data was web scraped, we will assume that the reason behind these missing values is that there was not a mean rating value for those particular observations (app). Thus, we will omit all of the NA values and continue to investigate only those apps for which we have ratings. 

Furthermore, here it is worth noting that the variable, `Genres` contains the same information in the `category` variable - the only difference being that the data is just displayed a bit differently. Therefore, as to avoid being redundant, we will only be using `category` in our analysis. As well, the information contained in the variable `Type` is also contained in `Price`. Since `Price` provides the prices of an app and `Type` simply denotes wether or not an app is paid or free. There is mostly likely a large linear dependency between these two variables as they measure the same thing, so we will use `Price` instead of `Type.` We will not deleting theese variables from the dataset as to maintain integrity, but will not examine them in our analysis.

As well, we also have many of predictors that are coded as characters in the dataset, so we decided to recode them as factors. We also have some variables that are coded as characters due to the existence of a particular a symbol (ex. $), we will recoded these into a format which will be usable for our analysis. 

Looking at the data, there are two variables related to the version, or iteration of the app as provided by the developers: `Current Ver` and `Android Ver`. Given that Google owns both Android and the Google Play Store, the company would likely be more interested in the Android version of the app. Furthermore, Android users are unlikely to be using other operating system's application stores, so a developer who is interested in creating apps for the Android market would gain more information through an examination of the compatibility of certain apps with a particular version of Android. Some data wrangling was necessary to make this variable suitable for analysis.

We also created a variable called `date_since`, which marks the number of days that the app has been updated since the day that the data was scraped on August 8, 2018. This will allow us to determine how recent the last update was for a particular app and provides some information related to the relative frequency of updates and how that may affect an app's rating.

```{r dates, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(date_new = mdy(`Last Updated`))
apps <- apps %>%
  mutate(date_since = mdy("August 8, 2018") - date_new)
```

There are also a number of variables which required releveling. Many of our predictors have multiple levels and these may have made our model too complicated. To reduce the probabiltiy that the model overfits the data and is too complicated, we will relevel our variables as follows:

Since our variable `Price` is currently not numeric and isn't coded into categories, we will relevel price into 3 categories: Free, Between 0 and 4.99 dollars, and greater than 5 dollars. 

```{r price-relevel, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(Price = case_when(Price == 0 ~ "Free", Price < 500 & Price > 0 ~ "Between $0 and $4.99", Price > 500 ~ "Greater than $5"))

apps <- apps %>%
  mutate(Price = fct_relevel(Price, "Free", "Between $0 and $4.99", "Greater than $5"))
```

Since our variable `Size` is currently very widely distributed, we will relevel size into 3 categories: Varies with Device, Less than 100, and Greater than 100.

```{r sizerelevel, warning = FALSE, message = FALSE}
apps$`Size`<- sub('k$', '', apps$`Size`)
apps$`Size`<- as.numeric(gsub("Varies with device", 0.001 , apps$`Size`))
apps <- apps %>%
  mutate(Size = case_when(Size == 0.001 ~ "Varies with device", Size < 100 ~ "Less than 100 MB", Size > 100 ~ "Greater than 100 MB"))
```

Since our variable `Installs` is currently very widely distributed, we will relevel installs into 3 categories: Less than 100, Between 100 and 1,000, Between 1,000 and 10,000, Between 10,000 and 100,000, and 100,000 or Greater.

```{r installs-relevel, warning = FALSE, message = FALSE}
apps$`Installs`<- as.numeric(gsub(",", "", apps$`Installs`))

apps <- apps %>%
  mutate(Installs = case_when(Installs < 100 ~ "Less than 100", Installs < 1000 & Installs >= 100 ~ "Between 100 and 1,000", Installs < 10000 & Installs >= 1000 ~ "Between 1,000 and 10,000", Installs < 100000 & Installs >= 10000 ~ "Between 10,000 and 100,000", Installs >= 100000 ~ "100,000 or Greater"))


apps <- apps %>%
  mutate(Installs = fct_relevel(Installs, "Less than 100", "Between 100 and 1,000", "Between 1,000 and 10,000", "Between 10,000 and 100,000", "100,000 or Greater"))

```

Our variable `Category` is extremely large and has many levels as shown in the graphiic below. To simplify this, we will create a new variable called `category_simp` with two levels: one for the top 6 categories ("FAMILY","GAME", "TOOLS","MEDICAL", "LIFESTYLE", and "FINANCE") and another for all the other categories. 

```{r category pop, warning = FALSE, message = FALSE}
apps %>%
  count(Category) %>%
  arrange(-n) %>%
  mutate(freq = n/sum(n))
```

```{r relevel cat, warning = FALSE, message = FALSE}
apps <- apps %>%
  mutate(category_simp = case_when( 
    apps$Category %in% c("FAMILY","GAME", "TOOLS","MEDICAL", "LIFESTYLE", "FINANCE") ~ "Top 6 Categories", 
    TRUE ~ "Others"))

apps %>%
  count(Category, category_simp) %>%
  arrange(desc(n))
```

As shown in the above table, we can see that the top 6 cateogies are in one level and the others are stored in another level. 

```{r relevel-andriod-ver}
apps %>%
  count(`Android Ver`) %>%
  arrange(desc(n))

apps <- apps %>%
  na.omit(`Android Ver`) %>%
  mutate(androidver_simp = case_when( 
    `Android Ver` %in% c(1,2,3,4) ~ "1-4", 
    `Android Ver` %in% c(5,6,7,8) ~ "5-8",
    TRUE ~ "Varies with Device"))

apps %>%
  count(`Android Ver`, androidver_simp) %>%
  arrange(desc(n))
```

```{r relevel-content}
apps <- apps %>%
  mutate(content_simp = case_when( 
    `Content Rating` == "Everyone" ~ "Everyone",
    `Content Rating` == "Teen" ~ "Teen",
    `Content Rating` == "Mature" ~ "Mature",
    `Content Rating` == "Everyone" ~ "Everyone",
    `Content Rating` == "Adults only" ~ "Adults only",
    `Content Rating` == "Unrated" ~ "Unrated"))

apps %>%
  count(`Content Rating`, content_simp) %>%
  arrange(desc(n))
```

#### Univariate Analysis

```{r rating-distribution, warning = FALSE, message = FALSE}
ggplot(data = apps, aes(x = Rating)) + 
  geom_histogram(binwidth = 0.1, fill = "blue") + 
  xlim(0,5) + labs(title = "Distribution of App Ratings")
```

```{r summary-stats, warning = FALSE, message = FALSE}
apps %>% 
  summarise(median(Rating), IQR(Rating))
```

```{r log reviews, warning = FALSE, message = FALSE}
apps <- apps %>% 
  mutate(log_reviews = log(Reviews)) 
ggplot(apps, aes(x = log_reviews)) + geom_histogram(binwidth = 0.5)
apps %>%
  summarise(median(log_reviews), max(log_reviews), IQR(log_reviews))
```

Above is the plot of the logged number of reviews for each application. We decided to log, due to an extreme skew seen in the plot of the original variable. 

FOR OTHER UNIVARIATE PLOTS ONLY INCLUDE THOSE IN FULL MODEL

Arrange the bars in the barplot of Categories in order of frequency. This will help the reader more easily distinguish which categories are the most common.

Which app has 78 million reviews?

Change the bin width on the histogram of date_since. It should be fine to just use the default bin width by R.

#### Bivariate Analysis

A bivariate analysis between variables will help understand the interaction between indidivual predictor variables and the response. 

```{r category-rating}
ggplot(data = apps, aes(x = category_simp, y = Rating)) + geom_boxplot() + coord_flip() +
  labs(title = "Relationship between Category and Rating") 
```

Although there is some variation in rating between app categories, the most telling aspect of this exploratory model is the outliers. It appears that some categories are more susceptible to outliers with low ratings. More over there are notable discrepancies between minimum boxplot rating among categories. 

```{r reviews-rating}
ggplot(data = apps, aes(x = Reviews, y = Rating)) + geom_point() +
  labs(title = "Relationship between Reviews and Rating", x = "Number of Reviews") 
```

Based on the scatterplot above, there is likely a relationship between number of reviews and app rating. As the number of reviews increased the app rating was concentrated at approximately 4.5 - which was consistent with apps holding smaller number of reviews. IS THIS RIGHT??

```{r installs-rating}
ggplot(data = apps, aes(x = Installs, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Installs and Rating", x = "Number of Installs") + coord_flip()
```

The boxplot above clearly shows a significant relationship between number of installs and rating. As the number of installs increases the IQR appears to decrease in conjunction. Moreover median rating also increases with number of installs. 

```{r type-rating}
ggplot(data = apps, aes(x = Type, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Type and Rating")
```

The boxplots for free and paid apps sport nearly identical median and IQR values. This tells us that whether an app is free or paid doesn't appear to have a major impact on the rating. Further analysis into the variation of rating among apps of different price levels is needed. 

#### Possible Interactions
ONLY INCLUDE THOSE IN FULL MODEL

## Section 2: Regression Analysis (includes the final model and discussion of assumptions)

### Model Process

The regression modeling technique we will use will be Multiple Linear Regression (MLR). Since we are exploring the effect of multiple predictor variables on our response, `rating`, it is apt that we use MLR to model our data. MLR allows us to see the effect of multiple predictors on a response and explore both the significance of each predictor on the response as well as the effect of each predictor on the response. As opposed to Simple Linear Regression, MLR allows us to measure the effect of multiple predictors on your response in one model - SLR only allows us to measure the effect of one predictor on the response in one model. This is very taxing and inefficient for the number of predictors we want to measure. As well, there may be interactions between these predictors that we will be unable to view using SLR. MLR allows us to both model and view the amalgamation of these predictors in their effects on the response variable. MLR from both an efficiency and relevancy perspective is much better suited to model our data as opposed to other methods. 

Our ultimate goal is to create the model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. We will attempt to choose a model using a minimization of both BIC and AIC as our criteria as this will allow us to calculate a precise prediction of our response variable while also removing extraneous predictors. We will use BIC and AIC as our selection criteria as it penalizes more for erroneous predictors as compared to adj. R-Squared. We will not use R-squared as a criteria for model selection. R squared increases strictly as the number of predictors increases and does not tell us if these additional predictors are significant or not. If we used r-squared we would always choose models with the largest numbers of predictors, which would not always produce the simplest, most accurate model. Unlike R squared, AIC, BIC, and adjusted R squared do penalize for insignificant predictors and can give us a better idea of which predictors actually contribute to the response variable. 

In order to find our final model, we will use a process of both forwards and backwards selection slowly adding a combination of relevant predictors into our model. We will then check the BIC and AIC values for each of these models and find the model with the lowest value overall, or the fewest predictors - this will be the model that most accurately predicts our response with the fewest number of predictors. We will then plot each predictor on the response to determine if the effect is relevant or if there are possible interactions between other variables. As well, we will need to consider potential outliers and extraneous values in our model. Using the distributions of standardized residuals and a calculation of Cook’s distance, we will attempt to determine those observations with high standardized residuals or cook’s distance and determine if those observations have a significant effect on our model. Lastly, we will need to find the VIF factor for each of our final predictors to see if there is any collinearity between them. A VIF greater than 10 would require us to explore possible ways to mitigate interactions between variables or consider dropping predictors are are too heavily correlated. 

### Model Selection

```{r full-model}
apps <- apps %>%
  na.omit(apps)
full_model <- lm(Rating ~ category_simp + log_reviews + Size + Installs + Price + `Content Rating` + androidver_simp + date_since, data = apps)
kable(tidy(full_model),format="html",digits=4)
```

```{r bic-back}
regfit_backward <- regsubsets(Rating ~ category_simp + log_reviews + Size + Installs + Price + `Content Rating` + androidver_simp + date_since, data = apps, method="backward")
sel_summary <- summary(regfit_backward)
coef(regfit_backward, which.min(sel_summary$bic))
```

```{r bic-forward}
regfit_forward <- regsubsets(Rating ~ category_simp + log_reviews + Size + Installs + Price + `Content Rating` + androidver_simp + date_since, data = apps, method="forward")
sel_summary <- summary(regfit_forward)
coef(regfit_forward, which.min(sel_summary$bic))
```

Given the BIC forwards and backwards selection our reduced linear model is: 

hat(mean rating) =  4.5124715854 x exp(0.0744108983(log_reviews)) - 0.3447278672(InstallsBetween 100 and 1,000) -0.6891663588 (InstallsBetween 1,000 and 10,000) - 0.8405412693(InstallsBetween 10,000 and 100,000) -1.0072240860(Installs 100,000 or Greater) + 0.0978316624(PriceBetween 0 and 4.99) -0.0001211124(date_since)

### Interactions & Our Updated Model

(finish this)

### Assumptions
(finish this)

### Model Assesment
(c/p code from regressions wehn we finish stuff)

### Model Interpretation
Will finish after we get model

## Section 3: Discussion and Limitations 


## Section 4: Conclusion


## Section 5: Additional Work

### References

Gupta, Lavanya. Kaggle. Jan. 2019, www.kaggle.com/lava18/google-play-store-apps?fbclid=IwAR36EMS2jg5fhPi-BQlX6Mv4MCk8YUm2XmyOLt0zsKkNyc9JK-JD7aLy-6I. Accessed 30 Oct. 2019. 

Dignan, Larry. “App Economy Expected to Be $120 Billion in 2019 as Small Screen Leads Digital Transformation Efforts.” ZDNet, ZDNet, 16 Jan. 2019, www.zdnet.com/article/app-economy-expected-to-be-120-billion-in-2019-as-small-screen-leads-digital-transformation-efforts/
