---
title: "Understanding Google Play Application Reviews"
subtitle: "Regression Analysis"
author: "RTime2Shine"
date: "11/20/19"
output: github_document
---

To Do:
- Check Bivaiate Plots
- finish interpretation interpret a few things
- final edits

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-packages, message=FALSE}
library(tidyverse)
library(broom)
library(knitr) 
library(skimr)
library(readr)
library(ggplot2)
library(stringr)
library(lubridate)
library(Sleuth3) 
library(ISLR)
library(leaps)
library(rms)
```

```{r load-data, message=FALSE}
apps <- read_csv("/cloud/project/02-data/googleplaystore.csv")
apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
apps$`Rating` <- as.numeric(apps$`Rating`)
```

### Research Question and Modeling Objective:

What are the relevant factors that affect the rating given for apps in the Google Play store?

Our ultimate goal is to create a model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. This will enable us to see which predictor variables interact with each other to effect the rating for a given app. We posit that examining such relationships will help developers understand what factors may influence an app's rating and use that information to create better applications for consumers. As well, conglomerates such as Google (whom this dataset is from) can use this information to more accurately display or promote apps that meet these characteristics or promote ads related to these apps and generate revenue. 

### Response Variable:

The response variable in our investigation is `Rating` which is the mean rating out of 5.0 for an application in the Google Play Store. This is a numeric variable. 

### Updated Exploratory Data Analysis

Below is some additional exploratory data analysis to further understand the response variable. 

```{r na}
skim(apps)
```

#### Data Wrangling

##### Removing Missing Values

Upon examining our predictor variables, it looks like there are occasionally one or two observations missing in the dataset, which does not raise lots of concern. It is worth noting, however, that 1474 of our response variable values are missing. This is roughly 10% of the data. Given that the data was web scraped, we will assume that the reason behind these missing values is that there was not a mean rating value for those particular observations (app). Thus, we will omit all of the NA values and continue to investigate only those apps for which we have ratings. 

Furthermore, here it is worth noting that the variable, `Genres` contains the same information in the `category` variable - the only difference being that the data is just displayed a bit differently. Therefore, as to avoid being redundant, we will only be using `category` in our analysis. We will not deleting the variable from the dataset as to maintain integrity.

```{r omit}
apps <- apps %>%
  na.omit(apps)
```

##### Recoding Variables

We also have a lot of predictors that are coded as characters in the dataset, so we will recode them as factors. We also have some variables that are coded as characters due to the existence of a particular a symbol (ex. $), we will also recode these into a format which will be usable for our analysis. 

```{r fact recode}
apps <- apps %>%
  mutate(Category = as.factor(Category)) %>%
  mutate(Size = as.factor(Size)) %>%
  mutate(Installs = as.factor(Installs)) %>%
  mutate(Type = as.factor(Type)) %>%
  mutate(`Content Rating` = as.factor(`Content Rating`)) %>%
  mutate(Genres = as.factor(Genres)) %>%
  mutate(Category = as.factor(Category))

apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
apps$Price <- as.numeric(gsub('[$.]', '', apps$Price))
```

Looking at the data, there are two variables related to the version, or iteration of the app as provided by the developers. Given that Google owns both Android and the Google Play Store, the company would likely be more interested in the Android version of the app. Furthermore, Android users are unlikely to be using other operating system's application stores, so a developer who is interested in creating apps for the Android market would gain more information through an examination of the compatibility of certain apps with a particular version of Android. Some data wrangling is necessary to make this variable suitable for analysis.

```{r version, results="hide"}
apps$`Android Ver`<-gsub("\\..*","",apps$`Android Ver`)
apps$`Android Ver`<-as.factor(apps$`Android Ver`)
apps$`Android Ver`[apps$`Android Ver` == "NaN"] <- NA
```

##### Creating `date_since`

Now, we're going to create a variable called `date_since`, which marks the number of days that the app has been updated since the day that the data was scraped on August 8, 2018. This will allow us to determine how recent the last update was for a particular app and provides some information related to the relative frequency of updates and how that may affect an app's rating. 

```{r dates}
apps <- apps %>%
  mutate(date_new = mdy(`Last Updated`))
apps <- apps %>%
  mutate(date_since = mdy("August 8, 2018") - date_new)
```

Now we are ready to do some preliminary analysis and visualization to have a more complete idea of the data we are working with. 

##### Releveling `Price`

Since our variable `Price` is currently not numeric and isn't coded into categories, it is best that we relevel and factorize this as to streamline our analysis. We will relevel price into 3 categories: Free, Between 0 and 4.99 dollars, and greater than 5 dollars. 

```{r price-relevel}
apps <- apps %>%
  mutate(Price = case_when(Price == 0 ~ "Free", Price < 500 & Price > 0 ~ "Between $0 and $4.99", Price > 500 ~ "Greater than $5"))

apps <- apps %>%
  mutate(Price = fct_relevel(Price, "Free", "Between $0 and $4.99", "Greater than $5"))
skim(apps, Price)
```

##### Releveling `Installs`

Since our variable `Installs` is currently very widely distributed, we will relevel this variable and create new bins as to streamline our analysis. We will relevel installs into 3 categories: Less than 100, Between 100 and 1,000, Between 1,000 and 10,000, Between 10,000 and 100,000, and 100,000 or Greater.

```{r installs-relevel}
apps$`Installs`<- as.numeric(gsub(",", "", apps$`Installs`))

apps <- apps %>%
  mutate(Installs = case_when(Installs < 100 ~ "Less than 100", Installs < 1000 & Installs >= 100 ~ "Between 100 and 1,000", Installs < 10000 & Installs >= 1000 ~ "Between 1,000 and 10,000", Installs < 100000 & Installs >= 10000 ~ "Between 10,000 and 100,000", Installs >= 100000 ~ "100,000 or Greater"))


apps <- apps %>%
  mutate(Installs = fct_relevel(Installs, "Less than 100", "Between 100 and 1,000", "Between 1,000 and 10,000", "Between 10,000 and 100,000", "100,000 or Greater"))

skim(apps, Installs)
```

##### Releveling `Size`

Since our variable `Size` is currently very widely distributed, we will relevel this variable and create new bins as to streamline our analysis. We will relevel size into 3 categories: Varies with Device, Less than 100, and Greater than 100.

```{r sizerelevel}
apps$`Size`<- sub('k$', '', apps$`Size`)
apps$`Size`<- as.numeric(gsub("Varies with device", 0.001 , apps$`Size`))
apps <- apps %>%
  mutate(Size = case_when(Size == 0.001 ~ "Varies with device", Size < 100 ~ "Less than 100 MB", Size > 100 ~ "Greater than 100 MB"))

```

#### Distribution of Response

```{r rating-distribution, message=FALSE}
ggplot(data = apps, aes(x = Rating)) + 
  geom_histogram(binwidth = 0.1, fill = "blue") + 
  xlim(0,5) + labs(title = "Distribution of App Ratings")
```

```{r summary-stats}
apps %>% 
  summarise(median(Rating), IQR(Rating))
```

As shown above, the distribution of our response variable, `rating` appears to be left-skewed. This will be important to take into account as we continue our analysis as it may require a transformation; however, our model is robust to departures in normality so we will continue. We determined to report median and IQR as our summary statistics because the distribution of `rating` appears to be left-skewed. The median rating of an app is approximately **4.3** and the IQR is **0.5**.

#### Univariate Analysis

We will now conduct a univariate analysis of all the possible predictor variables. For the purposes of our regression, we will be examining the following variables: `Category`, `Reviews`, `Size`, `date_since`, `Installs`, `Type`, `Price`, `Content Rating`, and `Android Version`. 

We have decided to not consider the other predictors as they are either irrelevant to our analysis due to the fact that they are simply indicator variables to distinguish the observations in our dataset - such as with `App` , or for redundancy and clarity as mentioned in the above sections in regards to `Current Version` and `Genres`.

##### Category

```{r count}
count(apps, Category) %>% 
  arrange(desc(n)) 
```

```{r category, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Category)) +
  geom_histogram(stat="count") +
  coord_flip()+
  labs(title = "Frequency of Categories") 
```

##### Reviews

```{r Reviews, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Reviews)) +
  geom_histogram(binwidth = 1000000) +
  labs(title = "Distribution of Reviews")

apps %>%
  summarise(median(Reviews)) 
apps %>%
  summarise(max(Reviews))
```
Reviews is definetley one of our most skewed predictors, as shown by the strong right skew in this histogram. This is because the median number of reviews is about 6,000, while we have a max review number of 78158306, which partially explains the skew. 

##### Size

```{r size, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Size)) +
  geom_histogram(stat="count", binwidth = 0.00025) +
  labs(title = "Distribution of App Sizes")
```
As shown by the distribution of app sizes, it is clear that most of our apps are less than 100MB, and there are also some that vary with device (meaning that no specific size information was available). There are very few apps that have sizes greater than 100 MB.

##### Installs

```{r installs, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Installs)) +
  geom_histogram(stat="count") +
  coord_flip()+
  labs(title = "Distribution of Installs")
```
The distribution for number of installs is also left skewed but not particularly abnormal. The majority of reviews have 100k installs or more, and then from there on, the number of observations for each level of installs decreases more than the previous one. Few apps have less than 100 installs.

##### Type

```{r type, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Type)) +
  geom_histogram(stat="count") +
  labs(title = "Distribution of Type (Paid or Unpaid)")

apps %>%
  count(Type) %>%
  mutate(proportion = n/sum(n))
```
The vast majority (93%) of apps on the Google Play store are free, while about 7% of the apps on the Google play store are paid. 

##### Price

```{r price, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Price)) +
  geom_histogram(stat="count")+
  coord_flip() +
  labs(title = "Distribution of Price (in USD)")
```
The distribution of price is, as expected, pretty similar to the distribution of `Type`. We see a right skewed distribution, with the majority of apps being free, while the ones that are paid are between 0 and 4.99, and a very small proportion of apps being greater than 5 dollars.

##### Content Rating

```{r univariate, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = `Content Rating`)) +
  geom_bar() +
  labs(title = "Frequency of Each Content Rating")
```
The distribution for content rating is pretty left skewed. The majority of apps are rated Everyone, while the next most occurring category is Teen. After that, we see even less apps that are rated mature.

##### Android Version

```{r android, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = `Android Ver`)) +
  geom_bar() +
  labs(title = "Distribution of Latest Compatible Android Version of App")
```
The distribution of Android version has a somewhat normal and unimodal shape. We can see that most of the apps are on version 4 , while there are also a significant number of apps that are on version 2. It is also important to note that there are a pretty good amount of apps whose android version varies with device. 

##### Date Since

```{r datesince, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = date_since)) +
  geom_histogram(stat= "count", binwidth = 3) +
  labs(title = "Frequency of Date Since Last Updated", subtitle = "Based off of number of days since August 1, 2018")
```
This is also another very right skewed variable. We can see that the majority of apps are last updated within 500 days of the scrape date, however there are some outliers that go up to even 3000 days since the scrape date. The plot is still unimodal.

#### Bivariate Analysis

To conduct a bivariate analysis, we will be making a pairs plot. 

```{r pairs-plot}
library(base)
#pairs(Rating ~ Reviews + Size + Installs + Price + date_since, data = apps, lower.panel = NULL)
```

Given the above pairs plot, we will be investigating some relationships more in depth: 

```{r category-rating}
ggplot(data = apps, aes(x = Category, y = Rating)) + geom_boxplot() + coord_flip() +
  labs(title = "Relationship between Category and Rating") 
```

Although there is some variation in rating between app categories, the most telling aspect of this exploratory model is the outliers. It appears that some categories are more susceptible to outliers with low ratings. More over there are notable discrepancies between minimum boxplot rating among categories. 

```{r reviews-rating}
ggplot(data = apps, aes(x = Reviews, y = Rating)) + geom_point() +
  labs(title = "Relationship between Reviews and Rating", x = "Number of Reviews") 
```

Based on the scatterplot above, there is likely **not** a relationship between number of reviews and app rating. As the number of reviews increased the app rating was concentrated at approximately 4.5 - which was consistent with apps holding smaller number of reviews. 

```{r installs-rating}
ggplot(data = apps, aes(x = Installs, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Installs and Rating", x = "Number of Installs") + coord_flip()
```

The boxplot above clearly shows a significant relationship between number of installs and rating. As the number of installs increases the IQR appears to decrease in conjunction. Moreover median rating also increases with number of installs. 

```{r type-rating}
ggplot(data = apps, aes(x = Type, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Type and Rating")
```

The boxplots for free and paid apps sport nearly identical median and IQR values. This tells us that whether an app is free or paid doesn't appear to have a major impact on the rating. Further analysis into the variation of rating among apps of different price levels is needed. 

#### Possible Interactions 

First, there is a possible interaction between content rating and categories. 

```{r int-content}
ggplot(apps, aes(x = Category, y = Rating, color = `Content Rating`)) + geom_point() + 
coord_flip() +
labs( title = "Relationship between Category and Rating", x ="Category", y = "Rating out of 5")
```

As shown in the plot above, there may be a correlation between having a lower content rating and being in a “family-friendly” category such as Family or game -  a clear example of this phenomenon is in the category dating. This interaction will have to be considered when building the model. As well, there is a clear interaction between other categories such as Mature or Teen being heavily represented among certain Categories. Secondly, there may be an interaction between number of reviews and rating. 

```{r int-reviews}
ggplot(apps, aes(x = Reviews, y = Rating, color = Reviews)) + 
  geom_point() +
  labs( title = "Relationship between Reviews and Rating", x ="# of Reviews ", y = "Rating out of 5")
```

As shown in this plot, as the number of reviews for an app increases, so does the rating generally. This is indicative of an app being popular so as there are more reviews there is most likely more polarization in the ratings. There is most likely some interaction between these two variables in the dataset. Thirdly, there may be an interaction between Type and Price. Since Type is an indicator measuring whether an app is paid or free, all apps that are free will be correlated with apps that have a price = 0 and apps that are paid will be correlated with apps that have a price greater than 0. 

```{r int-type}
ggplot(apps, aes(x = Type, y = Rating, color = Price)) + 
  geom_point() +
  labs( title = "Relationship between Type & Rating", x ="Type", y = "Rating out of 5")
```

This is further illustrated through the above plot, which clearly shows this interaction. These interactions along with any further ones we may find after our preliminary analysis will have to be explored further and considered when building our model. 

#### Correlation 

Since our variables are mostly categorical, we will not need to check a correlation matrix to determine if we need to remove any highly correlated variables. Instead, our preliminary analysis of interactions should suffice.  

### Explanation of the Model Process

The regression modeling technique we will use will be Multiple Linear Regression (MLR). Since we are exploring the effect of multiple predictor variables on our response, `rating`, it is apt that we use MLR to model our data. MLR allows us to see the effect of multiple predictors on a response and explore both the significance of each predictor on the response as well as the effect of each predictor on the response. As opposed to Simple Linear Regression, MLR allows us to measure the effect of multiple predictors on your response in one model - SLR only allows us to measure the effect of one predictor on the response in one model. This is very taxing and inefficient for the number of predictors we want to measure. As well, there may be interactions between these predictors that we will be unable to view using SLR. MLR allows us to both model and view the amalgamation of these predictors in their effects on the response variable. MLR from both an efficiency and relevancy perspective is much better suited to model our data as opposed to other methods. 

Our ultimate goal is to create the model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. We will attempt to choose a model using a minimization of both BIC and AIC as our criteria as this will allow us to calculate a precise prediction of our response variable while also removing extraneous predictors. We will use BIC and AIC as our selection criteria as it penalizes more for erroneous predictors as compared to adj. R-Squared. We will not use R-squared as a criteria for model selection. R squared increases strictly as the number of predictors increases and does not tell us if these additional predictors are significant or not. If we used r-squared we would always choose models with the largest numbers of predictors, which would not always produce the simplest, most accurate model. Unlike R squared, AIC, BIC, and adjusted R squared do penalize for insignificant predictors and can give us a better idea of which predictors actually contribute to the response variable. 

In order to find our final model, we will use a process of both forwards and backwards selection slowly adding a combination of relevant predictors into our model. We will then check the BIC and AIC values for each of these models and find the model with the lowest value overall, or the fewest predictors - this will be the model that most accurately predicts our response with the fewest number of predictors. We will then plot each predictor on the response to determine if the effect is relevant or if there are possible interactions between other variables. As well, we will need to consider potential outliers and extraneous values in our model. Using the distributions of standardized residuals and a calculation of Cook’s distance, we will attempt to determine those observations with high standardized residuals or cook’s distance and determine if those observations have a significant effect on our model. Lastly, we will need to find the VIF factor for each of our final predictors to see if there is any collinearity between them. A VIF greater than 10 would require us to explore possible ways to mitigate interactions between variables or consider dropping predictors are are too heavily correlated. 

### Final Model

#### Full Model

We will attempt to use MLR to predict an app's Rating given the aforementioned predictors. Here is the output of our initial model:

```{r full-model}
full_model <- lm(Rating ~ Category + Reviews + Size + Installs + Type + Price + `Content Rating` + `Android Ver` + date_since, data = apps)
kable(tidy(full_model),format="html",digits=4)
```

The full model output is shown above. 

#### BIC

First to select the correct model, we will perform both forward and backward selection using BIC:

```{r bic-back}
regfit_backward <- regsubsets(Rating ~ Category + Reviews + Size + Installs + Type + Price + `Content Rating` + `Android Ver` + date_since, data = apps, method="backward")
sel_summary <- summary(regfit_backward)
coef(regfit_backward, which.min(sel_summary$bic))
```

```{r bic-forward}
regfit_forward <- regsubsets(Rating ~ Category + Reviews + Size + Installs + Type + Price + `Content Rating` + `Android Ver` + date_since, data = apps, method="forward")
sel_summary <- summary(regfit_forward)
coef(regfit_forward, which.min(sel_summary$bic))
```

#### AIC 

Second, we will perform both forward and backward selection using AIC:

```{r aic-back}
apps <- apps %>%
  na.omit(apps)
null_model <- lm(Rating ~ 1, data = apps,)
regfit_backward <- step(full_model, direction = "backward")
```

```{r aic-forward}
regfit_forward <- step(null_model, scope = formula(full_model), 
                       direction = "forward")
```

```{r hybrid}
regfit_hybrid <- step(null_model, scope = formula(full_model), 
                      direction = "both")
```


#### Final Model

```{r final-model}
final_model <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews, data = apps)
kable(tidy(final_model),format="html",digits=3)
```

The model which gave us the fewest predictors is Backwards AIC. We felt that this model had the fewest predictors as compared to the others and provided us with the most economical and concise model. This model uses the predictors `Installs`, `Category`, `date_since`, `Android Ver`, and `Reviews` to predict our response: `Rating`.

### Exploring Interactions 

Now, we will conduct an f-test including the possible interaction effects in our model to determine if there are any interaction effects which are significant.

```{r interactions1}
reduced <- final_model
full  <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews + Installs*Category + Installs*`Android Ver` + Category*`Android Ver` + Category*Reviews + date_since*`Android Ver` + date_since*Reviews + `Android Ver`*Reviews, data = apps)
kable(anova(reduced, full), format="markdown", digits = 3)
```

Our hypotheses for this F-test is as follows:

Since the number of levels of our categorical variables are k > 2 we can use a nested F test to determine if any of these interaction effects are significant. 

Ho: B(1) = B(2) = ... = 0

Ha: Atleast one Beta_j is not equal to 0

In this case, our null hypothesis can essentially be read as there are no interactions between any of the variables in our model and our alternative can be read as that there is a significant interaction effect in the model. 

Since our p-value of 0 is less than our significance level of 0.05, we do have evidence that there is a statistically significant interaction present in the model. 

We will now test each individual variable's interaction effects to determine which are significant. Again, the hypotheses are consistent with the ones outlined above.

```{r int-installs}
reduced <- final_model
full  <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews + Installs*Category + Installs*`Android Ver`, data = apps)
kable(anova(reduced, full), format="markdown", digits = 3)
```
Since our p-value of 0 is less than our significance level of 0.05, we do have evidence that there is a statistically significant interaction between both Installs and Category and Installs and Android Version. 


```{r int-category}
reduced <- final_model
full  <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews + Category*`Android Ver` + Category*Reviews, data = apps)
kable(anova(reduced, full), format="markdown", digits = 3)
```
Since our p-value of 0 is less than our significance level of 0.05, we do have evidence that there is a statistically significant interaction between both Category and Reviews and Category and Android Version. 

```{r int-date_since}
reduced <- final_model
full  <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews + date_since*`Android Ver` + date_since*Reviews, data = apps)
kable(anova(reduced, full), format="markdown", digits = 3)
```
Since our p-value of 0 is less than our significance level of 0.05, we do have evidence that there is a statistically significant interaction between both date_since and Reviews and date_since and Android Version. 

```{r int-androidver}
reduced <- final_model
full  <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews + `Android Ver`*Reviews, data = apps)
kable(anova(reduced, full), format="markdown", digits = 3)
```
Since our p-value of 0 is less than our significance level of 0.05, we do have evidence that there is a statistically significant interaction between Reviews and Android Version. 

Since we found all these interactions to be significant, our new final model is as follows:

```{r finalmodel2}
final_model2 <- lm(Rating ~ Installs + Category + date_since + `Android Ver` + Reviews + Installs*Category + Installs*`Android Ver` + Category*`Android Ver` + Category*Reviews + date_since*`Android Ver` + date_since*Reviews + `Android Ver`*Reviews, data = apps)
kable(tidy(final_model2),format="html",digits=3)
```

### Final Model Assumptions

Because we have conducted Multiple Linear Regression, the model assumptions we will check are Linearity, Constant Variance, Normality, and Independence. 

#### Linearity

The Linearity Assumptions assumes that the response variable has a linear relationship with the predictor variables used in the final model. To assess linearity, we look at the plots created in the Exploratory Data Analysis. None of these plots seem to have a non-linear relationship such as one that would be polynomial; however, some are quite skewed but this should not matter for the purposes of our model. To illustrate this, we will re-display the bivariate plots with our final predictor variables. 

```{r linearity}
ggplot(data = apps, aes(x = Installs, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Installs and Rating")

ggplot(data = apps, aes(x = Category, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Category and Rating")

ggplot(data = apps, aes(x = `Android Ver`, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Android Version and Rating")

ggplot(apps, aes(x = date_since, y = Rating)) + geom_point() +
labs(title = "Relationship between Date Since Last Update and Rating", x ="Date Since Last Update", y = "Rating")

ggplot(apps, aes(x = Reviews, y = Rating)) + geom_point() +
labs(title = "Relationship between # of Reviews and Rating", x ="# of Reviews", y = "Rating")
```

As shown in the plots above, almost all the variables appear to satisfy the linearity assumption. `Installs` is the only variable that appears to have somewhat of a parabolic, or non-linear relationship with Rating; however, due to the fact that we have included interaction effects between it and other predictors in our model it is most likely the case that this is insignificant. Otherwise, all of our predictors appear to have strong and non-curvilinear relationships clearly visible through the plot. 

#### Constant Variance

The Constant Variance Assumption assumes that the regression variance is the same for all of the predictor variables in the model. To test this assumption, we will plot the residual values against predictors.

```{r resid-plots}
apps <- apps %>% 
  mutate(predicted = predict.lm(final_model2), residuals = resid(final_model2)) #mutates residuals to the model
ggplot(data=apps,aes(x=predicted, y=residuals)) + #plots residuals vs predictors
  geom_point() + 
  geom_hline(yintercept=0,color="red") +
  labs(title="Residuals vs. Predicted Values")
```

As shown, it appears that the constant variance assumption is violated. There appears to be a clear pattern in our residuals. Since our response was normally distributed at the beginning of our EDA, we did not see the need for a transformation of our response; however, it may be the case that there is a need for more, higher-order interaction terms. Since our model satisfies the linearity assumption and also relatively satisfies our normality assumption it is reasonable to assume that this is not a great cause for concern. 

#### Normality

The Normality Assumption assumes that for any given set of predictors, the response variable, `ratings`, follows a Normal distribution around its mean. To test this, we will make a Normal QQ plot. 

```{r norm-qq}
 ggplot(data = apps, mapping = aes(sample = residuals)) + 
   stat_qq() + 
   stat_qq_line() +
   labs(title = "Normal QQ Plot of Residuals")
 
 ggplot(data = apps, mapping = aes(x = residuals)) + 
   geom_histogram()+
   labs(title = "Distribution of Residuals")
```

As shown, our residuals are relatively normally distributed in our histogram. The center is around 0 which is good; however there is a slight left skew. As well, our Normal-QQ plot doesn't closely match the idea line at the beginning of the more negative values. It may be the case that there are many outliers or high leverage points in our model that are causing this skew and if they were removed then we wouldn't see this. However, since our distribution of residuals is relatively normal and the Normal QQ Plot is mostly following the trend line it is reasonable to assume that this assumption is satisfied due to the robustness f our model. In our model Assessment, we can further fix these issues and improve our model. 

#### Independence

The Independence Assumption assumes that all observations in the data used to construct the model are independent of each other. Given that each observation and is not dependent on the time frame or location of collection for its mean rating. The observations are independent of each other and thus the Independence Assumption is maintained. 

### Model Assessment

#### Leverage

According to lecture, the threshold we should use for determining if observations are high leverage points is: hi > (2(p+1))/n

```{r leverage}
apps_output <- augment(final_model2) %>%
  mutate(obs_num = row_number())

leverage_threshold <- (2*(12+1))/nrow(apps)
ggplot(data = apps_output, aes(x = obs_num,y = .hat)) + 
  geom_point(alpha = 0.3) + 
  geom_hline(yintercept = leverage_threshold,color = "red")+
  labs(x = "Observation Number",y = "Leverage",title = "Leverage") +
  geom_text(aes(label=ifelse(.hat > leverage_threshold, as.character(obs_num), "")), nudge_x = 4)
```

Let’s filter to see the total # of points that crossed the threshold.

```{r filterlev}
apps_output %>% filter(.hat > leverage_threshold) %>%
  select(Installs, Category, date_since, Android.Ver, Reviews)
```

As shown, there are 8,134 high leverage points. This is problematic but before we assess our model's strength we should check cook's distance. 

#### Cook's Distance

Now let’s check how many of these points cross our threshold for Cook’s Distance (Di > 1) to determine if these high leverage points do in fact have a significant influence on our model coefficients.

```{r cooks-distance}
ggplot(data = apps_output, aes(x = obs_num, y = .cooksd)) + 
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=1,color = "red")+
  labs(x= "Observation Number",y = "Cook's Distance",title = "Cook's Distance") +
  geom_text(aes(label = ifelse(.hat>1,as.character(obs_num),"")))
```

Let's check to see the number of points that violated this threshold. 

```{r filtercook}
apps_output %>% filter(.cooksd > 1) %>%
  select(Installs, Category, date_since, Android.Ver, Reviews)
```

As shown, only one of these high leverage points violated Cook's Distance - so it is most likely the case that although outliers are present, our model still is relatively strong in predicting Rating and that there isn't a cause for concern. It is likely that none of these high leverage points have a significant influence on the model coefficients.

#### Standardized Residuals
Now, let’s plot our standardized residuals to see if there are any points which break the threshold |resid std.| > 2.

```{r standardresid-predicted}
ggplot(data = apps_output, aes(x = .fitted,y = .std.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept = 0,color = "red") +
  geom_hline(yintercept = -2,color = "red",linetype = "dotted") +
  geom_hline(yintercept = 2,color = "red",linetype = "dotted") +
  labs(x ="Predicted Value",y ="Standardized Residuals",title = "Standardized Residuals vs. Predicted") +
  geom_text(aes(label = ifelse(abs(.std.resid) >2,as.character(obs_num),"")), nudge_x = 0.08)
```

As depicted the plot, there are a number of points which violate this threshold, so let’s filter the data to get an exact number.

```{r filtersd}
apps_output %>% filter(abs(.std.resid) > 2) %>%
  select(Installs, Category, date_since, Android.Ver, Reviews)
```

As illustrated, there are 474 observations with a standardized residual greater than +/- 2. These observations are considered to have standardized residuals with large magnitude.

```{r residgram}
ggplot(data = apps_output, aes(x = .std.resid)) +
  geom_vline(xintercept = 2,color = "red") +
  geom_vline(xintercept = -2,color = "red") +
  geom_histogram() +
  labs(x = "Residuals")
```

```{r percent}
percentage <- (474/nrow(apps)) * 100
print(percentage)
```

The proportion of observations that have standardized residuals with magnitude > 2 is about 0.50711, or 5.071% of the observations. Since this proportion is almost less than 5%, it is most likely the case that it is not statistically significant and is relatively small. Therefore, there is most likely not a concern with the number of observations flagged as having standardized residuals with large magnitude as the proportion of these residuals is relatively small. Although our proportion is somewhat greater than 5% it is still most likely the case that these flagged residuals are a relatively small proportion of the data, as well since only 1 of our leverage points was cause for concern, it is most likely the case that our model is sound. As well, it is most likely these 5% of observations which may have caused the skew in our residual variance. 

### VIF

We will check the VIF of our model without interactions:
```{r vif}
tidy(vif(final_model))
```

`CategoryAUTO_AND_VEHICLES`, `CategoryFAMILY`, `CategoryGAME`, `CategoryTOOLS`, `Android Ver2`, `Android Ver4`, & `Android Ver-Varies with device` all have VIF > 10, indicating concern with multicollinearity. 

Since these two terms were interacted in our final model with interactions, there are no longer any concerns with highly correlated variables in our model. 

### Interpretations and Findings


In looking at the models and exploratory data analysis in the above sections, there are some interesting findings worth noting.

For instance, the plot of the response variable, `rating`, is incredibly skewed with a peak centered between a mean rating of 4 and 4.5. It would be incredibly interesting to research if people tend to under vote or if people who don't like an app just don't vote. Exploring the human decision making element in the reviewing process would aide in the understanding of this material. 

It is also worth noting the high intercept values (4.8261579 and 4.7270722) in the final model (with and without interaction terms). Given that a rating, and therefore the mean value of rating, can be no higher than 5, these intercept values are incredibly high.

There are some coefficients in our model that seem to make the ratings really high. A change in 1 unit of rating is quite a lot, so the coefficient of categoryHealth_and_Fitness is quite large at 1.4202378. This means that if an app is health and fitness, we expect the ratings to increase by 1.420 on average, holding all else constant. There are a few others like this like categoryFOOD_AND_DRINK and cateoryLIFESTYLE. For categoryFOOD_AND_DRINK, if an app is a food and drink app we expect the ratings to increase by 1.0244280 on average, holding all else constant. If an app is in the health and lifestyle category, we expect the ratings to increase by 1.1011342, holding all else constant.

Additionally, there are some factors that make the ratings decrease. The coefficient for InstallsBetween 1,000 and 10,000 leads to a -0.858 decrease in expected rating, on average, holding all else constant. Additionally, another aspect that seems to really affect rating is if an app is in the dating category- if it is, we expect the rating to decrease by 3.622, holding all else constant.

