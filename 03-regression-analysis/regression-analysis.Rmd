---
title: "Understanding Google Play Application Reviews"
subtitle: "Regression Analysis"
author: "RTime2Shine"
date: "11/20/19"
output: github_document
---

To Do:
- Edit current version (Lukengu) 
- Edit Date Since Update (Sara) DONE
- Size should be in bins (lukengu)
- Make the model (sanjay)
- Univariate Analysis (Zoe)
- Explanation of Model Process (Sanjay)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Your regression analysis results go here. At a minimum, the regression analysis should include the following: 

- Description of the response variable
- Updated exploratory data analysis, incorporating any feedback from the proposal (Zoe, Sara, Lukengu)
  - Univariate analysis, include the changed variables, explain why we are taking out what we are taking out, response vs all predictor variables. 
- Explanation of the modeling process and why you chose those metohds, incorporating any feedback from the proposal (Sanjay)
- Output of the final model (Sanjay)
- Discussion of the assumptions for the final model (Zoe)
- Interpretations / interesting findings from the model coefficients (Sara and Sanjay)
- Additional work of other models or analylsis not included in the final model.(Sanjay)

```{r load-packages, message=FALSE}
library(tidyverse)
library(broom)
library(knitr) 
library(skimr)
library(readr)
library(ggplot2)
library(stringr)
library(lubridate)
```

```{r load-data, message=FALSE}
apps <- read_csv("/cloud/project/02-data/googleplaystore.csv")
apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
apps$`Rating` <- as.numeric(apps$`Rating`)
```

### Research Question and Modeling Objective:

What are the relevant factors that affect the rating given for apps in the Google Play store?

Our ultimate goal is to create a model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. This will enable us to see which predictor variables interact with each other to effect the rating for a given app. We posit that examining such relationships will help developers understand what factors may influence an app's rating and use that information to create better applications for consumers. As well, conglomerates such as Google (whom this dataset is from) can use this information to more accurately display or promote apps that meet these characteristics or promote ads related to these apps and generate revenue. 

### Response Variable:

The response variable in our investigation is `Rating` which is the mean rating out of 5.0 for an application in the Google Play Store. This is a numeric variable. 

### Updated Exploratory Data Analysis

Below is some additional exploratory data analysis to further understand the response variable. 

```{r na}
skim(apps)
```

#### Data Wrangling

##### Removing Missing Values

Upon examining our predictor variables, it looks like there are occasionally one or two observations missing in the dataset, which does not raise lots of concern. It is worth noting, however, that 1474 of our response variable values are missing. This is roughly 10% of the data. Given that the data was web scraped, we will assume that the reason behind these missing values is that there was not a mean rating value for those particular observations (app). Thus, we will omit all of the NA values and continue to investigate only those apps for which we have ratings. 

Furthermore, here it is worth noting that the variable, `Genres` contains the same information in the `category` variable - the only difference being that the data is just displaid a bit differently. Therefore, as to avoid being redundant, we will only be using `category` in our analysis. We will not deleting the variable from the dataset as to maintain integrity.

```{r omit}
apps <- apps %>%
  na.omit(apps)
```

##### Recoding Variables

We also have a lot of predictors that are coded as characters in the dataset, so we will recode them as factors. We also have some variables that are coded as characters due to the existence of a particular a symbol (ex. $), we will also recode these inot a format which will be useable for our analysis. 

```{r fact recode}
apps <- apps %>%
  mutate(Category = as.factor(Category)) %>%
  mutate(Size = as.factor(Size)) %>%
  mutate(Installs = as.factor(Installs)) %>%
  mutate(Type = as.factor(Type)) %>%
  mutate(`Content Rating` = as.factor(`Content Rating`)) %>%
  mutate(Genres = as.factor(Genres)) %>%
  mutate(Category = as.factor(Category))

apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
apps$Price <- as.numeric(gsub('[$.]', '', apps$Price))
```

Looking at the data, there are two variables related to the version, or iteration of the app as provided by the developers. Given that Google owns both Android and the Google Play Store, the company would likely be more interested in the Android version of the app. Furthermore, Android users are unlikely to be using other operating system's application stores, so a developer who is interested in creating apps for the Andriod market would gain more information through an examination of the compatibility of certain apps with a particular version of Android. Some data wrangling is necessary to make this variable suitable for analysis.

```{r version, results="hide"}
apps$`Android Ver`<-gsub("\\..*","",apps$`Android Ver`)
apps$`Android Ver`<-as.factor(apps$`Android Ver`)
apps$`Android Ver`[apps$`Android Ver` == "NaN"] <- NA
```

##### Creating `date_since`

Now, we're going to create a variable called `date_since`, which marks the number of days that the app has been updated since the day that the data was scraped on August 8, 2018. This will allow us to determine how recent the last update was for a particular app and provides some information related to the relative frequency of updates and how that may affect an app's rating. 

```{r dates}
apps <- apps %>%
  mutate(date_new = mdy(`Last Updated`))
apps <- apps %>%
  mutate(date_since = mdy("August 8, 2018") - date_new)
```

Now we are ready to do some preliminary analysis and vizualization to have a more complete idea of the data we are working with. 

##### Releveling `Price`

Since our variable `Price` is currently not numeric and isn't coded into categories, it is best that we relevel and factorize this as to streamline our analysis. We will relevel price into 3 categories: Free, Between 0 and 4.99 dollars, and greater than 5 dollars. 

```{r price-relevel}
apps <- apps %>%
  mutate(Price = case_when(Price == 0 ~ "Free", Price < 500 & Price > 0 ~ "Between $0 and $4.99", Price > 500 ~ "Greater than $5"))

apps <- apps %>%
  mutate(Price = fct_relevel(Price, "Free", "Between $0 and $4.99", "Greater than $5"))
skim(apps, Price)
```

##### Releveling `Installs`

Since our variable `Installs` is currently very widely distributed, we will relevel this variable and create new bins as to streamline our analysis. We will relevel installs into 3 categories: Less than 100, Between 100 and 1,000, Between 1,000 and 10,000, Between 10,000 and 100,000, and 100,000 or Greater.

```{r installs-relevel}
apps$`Installs`<- as.numeric(gsub(",", "", apps$`Installs`))

apps <- apps %>%
  mutate(Installs = case_when(Installs < 100 ~ "Less than 100", Installs < 1000 & Installs >= 100 ~ "Between 100 and 1,000", Installs < 10000 & Installs >= 1000 ~ "Between 1,000 and 10,000", Installs < 100000 & Installs >= 10000 ~ "Between 10,000 and 100,000", Installs >= 100000 ~ "100,000 or Greater"))


apps <- apps %>%
  mutate(Installs = fct_relevel(Installs, "Less than 100", "Between 100 and 1,000", "Between 1,000 and 10,000", "Between 10,000 and 100,000", "100,000 or Greater"))

skim(apps, Installs)
```

##### Releveling `Size`

Since our variable `Size` is currently very widely distributed, we will relevel this variable and create new bins as to streamline our analysis. We will relevel size into 3 categories: Varies with Device, Less than 100, and Greater than 100.

```{r sizerelevel}
apps$`Size`<- sub('k$', '', apps$`Size`)
apps$`Size`<- as.numeric(gsub("Varies with device", 0.001 , apps$`Size`))
apps <- apps %>%
  mutate(Size = case_when(Size == 0.001 ~ "Varies with device", Size < 100 ~ "Less than 100 MB", Size > 100 ~ "Greater than 100 MB"))

```

#### Distribution of Response

```{r rating-distribution, message=FALSE}
ggplot(data = apps, aes(x = Rating)) + 
  geom_histogram(binwidth = 0.1, fill = "blue") + 
  xlim(0,5) + labs(title = "Distribution of App Ratings")
```

```{r summary-stats}
apps %>% 
  summarise(median(Rating), IQR(Rating))
```

As shown above, the distribution of our response variable, `rating` appears to be left-skewed. This will be important to take into account as we continue our analysis as it may require a transformation; however, our model is robust to departures in normality so we will continue. We determined to report median and IQR as our summary statistics because the distribution of `rating` appears to be left-skewed. The median rating of an app is approximately **4.3** and the IQR is **0.5**.

#### Univariate Analysis

We will now conduct a univariate analysis of all the possible predictor variables. For the purposes of our regression, we will be examining the following variables: `Category`, `Reviews`, `Size`, `date_since`, `Installs`, `Type`, `Price`, `Content Rating`, and `Android Version`. 

We have decided to not consider the other predictors as they are either irrelevant to our analysis due to the fact that they are simply indicator variables to distinguish the observations in our dataset - such as with `App` , or for redundancy and clarity as mentioned in the above sections in regards to `Current Version` and `Genres`.

##### Category

```{r count}
count(apps, Category) %>% 
  arrange(desc(n)) 
```

```{r category, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Category)) +
  geom_histogram(stat="count") +
  coord_flip()+
  labs(title = "Frequency of Categories") 
```

##### Reviews

```{r Reviews, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Reviews)) +
  geom_histogram(binwidth = 1000000) +
  labs(title = "Distribution of Reviews")
```

##### Size

```{r size, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Size)) +
  geom_histogram(stat="count", binwidth = 0.00025) +
  labs(title = "Distribution of App Sizes")
```

##### Installs

```{r installs, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Installs)) +
  geom_histogram(stat="count") +
  coord_flip()+
  labs(title = "Distribution of Installs")
```


##### Type

```{r type, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Type)) +
  geom_histogram(stat="count") +
  labs(title = "Distribution of Type (Paid or Unpaid)")
```

##### Price

```{r price, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = Price)) +
  geom_histogram(stat="count")+
  coord_flip() +
  labs(title = "Distribution of Price (in USD)")
```

##### Content Rating

```{r univariate, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = `Content Rating`)) +
  geom_bar() +
  labs(title = "Frequency of Each Content Rating")
```

##### Andriod Version

```{r android, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = `Android Ver`)) +
  geom_bar() +
  labs(title = "Dsitribution of Latest Compatible Andriod Version of App")
```

##### Date Since

```{r datesince, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
ggplot(data = apps, aes(x = date_since)) +
  geom_histogram(stat= "count", binwidth = 3) +
  labs(title = "Frequency of Date Since Scrapping")
```

#### Bivariate Analysis

To conduct a bivariate analysis, we will be making a pairs plot. 

```{r pairs-plot}
library(base)
#pairs(Rating ~ Reviews + Size + Installs + Price + date_since, data = apps, lower.panel = NULL)
```

Given the above pairs plot, we will be investigating some relationships more in depth: 

```{r category-rating}
ggplot(data = apps, aes(x = Category, y = Rating)) + geom_boxplot() + coord_flip() +
  labs(title = "Relationship between Category and Rating") 
```

Although there is some variation in rating between app categories, the most telling aspect of this exploratory model is the outliers. It appears that some categories are more suspectible to outliers with low ratings. More over there are notable discrepancies between minimum boxplot rating among categories. 

```{r reviews-rating}
ggplot(data = apps, aes(x = Reviews, y = Rating)) + geom_point() +
  labs(title = "Relationship between Reviews and Rating", x = "Number of Reviews") 
```

Based on the scatterplot above, there is likely **not** a relationship between number of reviews and app rating. As the number of reviews increased the app rating was concentrated at approximately 4.5 - which was consistent with apps holding smaller number of reviews. 

```{r installs-rating}
ggplot(data = apps, aes(x = Installs, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Installs and Rating", x = "Number of Installs") + coord_flip()
```

The boxplot above clearly shows a significant relationship between number of installs and rating. As the number of installs increases the IQR appears to decrease in conjunction. Moreover median rating also increases with number of installs. 

```{r type-rating}
ggplot(data = apps, aes(x = Type, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Type and Rating")
```

The boxplots for free and paid apps sport nearly identical median and IQR values. This tells us that whether an app is free or paid doesn't appear to have a major impact on the rating. Further analysis into the variation of rating among apps of different price levels is needed. 

#### Possible Interactions 

First, there is a possible interaction between content rating and categories. 

```{r int-content}
ggplot(apps, aes(x = Category, y = Rating, color = `Content Rating`)) + geom_point() + 
coord_flip() +
labs( title = "Relationship between Category and Rating", x ="Category", y = "Rating out of 5")
```

As shown in the plot above, there may be a correlation between having a lower content rating and being in a “family-friendly” category such as Family or game -  a clear example of this phenomenon is in the category dating. This interaction will have to be considered when building the model. As well, there is a clear interaction between other categories such as Mature or Teen being heavily represented among certain Categories. Secondly, there may be an interaction between number of reviews and rating. 

```{r int-reviews}
ggplot(apps, aes(x = Reviews, y = Rating, color = Reviews)) + 
  geom_point() +
  labs( title = "Relationship between Reviews and Rating", x ="# of Reviews ", y = "Rating out of 5")
```

As shown in this plot, as the number of reviews for an app increases, so does the rating generally. This is indicative of an app being popular so as there are more reviews there is most likely more polarization in the ratings. There is most likely some interaction between these two variables in the dataset. Thirdly, there may be an interaction between Type and Price. Since Type is an indicator measuring wether an app is paid or free, all apps that are free will be correlated with apps that have a price = 0 and apps that are paid will be correlated with apps that have a price greater than 0. 

```{r int-type}
ggplot(apps, aes(x = Type, y = Rating, color = Price)) + 
  geom_point() +
  labs( title = "Relationship between Type & Rating", x ="Type", y = "Rating out of 5")
```

This is further illustrated through the above plot, which clearly shows this interaction. These interactions along with any further ones we may find after our preliminary analysis will have to be explored further and considered when building our model. 

#### Correlation 

Since our variables are mostly categorial, we will not need to check a correlation matrix to determine if we need to remove any highly correlated variables. Instead, our preliminary analysis of interactions should suffice.  

### Explanation of the Model Process

### Final Model

### Final Model Assumptions

Becuase we have conducted Multipule Linear Regression, the model assumptions we will check are Linearity, Constant Variance, Normality, and Independence. 

#### Linearity

The Linearity Assumptions assumes that the response variable has a linear relationship with the predictor variables used in the final model. To assess linearity, we look at the plots created in the Exploratory Data Analysis. None of these plots seem to have a non-linear relationship such as a polynomial, however, some are quite skewed.

#### Constant Variance

The Constant Variance Assumption assumges that the regression variance is the same for all of the predictor variables in the model. To test this assumption, we will plot the residual values against predictors.

```{r resid-plots}
#apps <- apps %>% 
  #mutate(predicted = predict.lm(model), residuals = resid(model)) #mutates residuals to the model
#ggplot(data=apps,aes(x=predicted, y=residuals)) + #plots residuals vs predictors
  #geom_point() + 
 # geom_hline(yintercept=0,color="red") +
  #labs(title="Residuals vs. Predicted Values")

#make plots with resid as the y and the predictor values as the x. 
```

#### Normality

The Normality Assumption assumes that for any given set of predictors, the response variable, `ratings`, follows a Normal distribution around its mean. To test this, we will make a Normal QQ plot. 

```{r norm-qq}
# ggplot(data = apps, mapping = aes(sample = resid)) + 
#   stat_qq() + 
#   stat_qq_line() +
#   labs(title = "Normal QQ Plot of Residuals")
# 
# ggplot(data = apps, mapping = aes(x = resid)) + 
#   geom_histogram()+
#   labs(title = "Distribution of Residuals")
```

#### Independence

The Independence Assumption assumes that all observations in the data used to construct the model are independent of each other. Given that each observation and is not dependent on the time frame or location of collection for its mean rating. The observations are independent of each other and thus the Independence Assumption is maintained. 

### Interpretations and Findings
- Interpretations / interesting findings from the model coefficients

### Aditional Model Work
- Additional work of other models or analylsis not included in the final model.
