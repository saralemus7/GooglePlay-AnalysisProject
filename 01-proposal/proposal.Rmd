---
title: "PROJECT TITLE"
subtitle: "Project Proposal"
author: "RTime2Shine"
date: "October 29, 2019"
output: github_document
---

## Section 1. Introduction




## Section 2. Analysis plan
```{r load-packages}
library(tidyverse)
library(broom)
library(knitr) 
library(ggplot2)
```

```{r load-data}
apps <- read_csv("/cloud/project/02-data/googleplaystore.csv")
apps <- apps %>%
  na.omit(apps)
```



## Section 3. Regression Analysis Plan

```{r rating-distribution}
ggplot(data = apps, aes(x = Rating)) + geom_histogram(binwidth = 0.1, fill = "blue") + xlim(0,5) +
  labs(title = "Distribution of App Rating")
```

```{r summary-stats}
apps %>% 
  summarise(median(Rating), IQR(Rating))
```

```{r}
count(apps, Category) %>% 
  arrange(desc(n)) 
```

We determined median and IQR as our summary statistics because the distribution of `rating` appears to be slightly left-skewed. The median rating of an app is approximately **4.3** and the IQR is **0.5**

- In the possible interactions below, we can also see the relationship between our responsible variable `rating` and predictor variables of interest. 



### Possible Interactions
Since our question of interest is measuring the effect of various qualities of an app on its rating, there are a number of interactions within our predictor variables to consider. First, there is a possible interaction between content rating and categories. 

```{r int-content}
ggplot(apps, aes(x = Category, y = Rating, color = `Content Rating`)) + geom_point() +
labs( title = "Relationship between Category and Rating", x ="Category", y = "Rating out of 5")
```

As shown in the plot above, there may be a correlation between having a lower content rating and being in a “family-friendly” category such as Family or game. This interaction will have to be considered when building the model. As well, there is a clear interaction between other categories such as Mature or Teen being heavily represented among certain Categories. Secondly, there may be an interaction between number of reviews and installs. 

```{r int-installs}
ggplot(apps, aes(x = Reviews, y = Rating, color = Installs)) + geom_point() +
labs( title = "Relationship between Reviews and Rating", x ="# of Reviews ", y = "Rating out of 5")
```

As shown in this plot, as the number of reviews for an app increases, so does the number of installs. This is indicative of an app being popular so there is most likely some interaction between these two variables in the dataset. Thirdly, there may be an interaction between Type and Price. Since Type is an indicator measuring wether an app is paid or free, all apps that are free will be correlated with apps that have a price = 0 and apps that are paid will be correlated with apps that have a price greater than 0. 

```{r int-type}
ggplot(apps, aes(x = Type, y = Rating, color = Price)) + geom_point() +
labs( title = "Relationship between Type & Rating", x ="Type", y = "Rating out of 5")
```

This is further illustrated through the above plot, which clearly shows this interaction. These interactions along with any further ones we may find after our preliminary analysis will have to be explored further and considered when building our model. 

### Model Selection

Our ultimate goal is to create the model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. We will attempt to choose a model using a minimization of BIC as our criteria as this will allow us to calculate a precise prediction of our response variable while also removing extraneous predictors. We will use BIC as the selection criteria as it penalizes more for erroneous predictors. We will not use R-squared as a criteria for model selection. R squared increases strictly as the number of predictors increases and does not tell us if these additional predictors are significant or not. If we used r-squared we would always choose models with the largest numbers of predictors, which would not always produce the simplest, most accurate model. Unlike R squared, AIC, BIC, and adjusted R squared do penalize for insignificant predictors and can give us a better idea of which predictors actually contribute to the response variable. In order to find our final model, we will use a process of backwards selection slowly adding a combination of relevant predictors into our model. We will then check the BIC values for each of these models as well as the adj-R squared values and find the model with the highest value- this will be the model that most accurately predicts our response with the fewest number of predictors. We will then plot each predictor on the response to determine if the effect is relevant or if there are possible interactions between other variables. As well, we will need to consider potential outliers and extraneous values in our model. Using the distributions of standardized residuals and a calculation of Cook’s distance, we will attempt to determine those observations with high standardized residuals or cook’s distance and determine if those observations have a significant effect on our model. Lastly, we will need to find the VIF factor for each of our final predictors to see if there is any collinearity between them. A VIF greater than 10 would require us to explore possible ways to mitigate interactions between variables or consider dropping predictors are are too heavily correlated. 

### Use of Model and Reason

The regression modeling technique we will use will be Multiple Linear Regression (MLR). Since we are exploring the effect of multiple predictor variables on our response, `rating`, it is apt that we use MLR to model our data. MLR allows us to see the effect of multiple predictors on a response and explore both the significance of each predictor on the response as well as the effect of each predictor on the response. As opposed to Simple Linear Regression, MLR allows us to measure the effect of multiple predictors on your response in one model - SLR only allows us to measure the effect of one predictor on the response in one model. This is very taxing and inefficient for the number of predictors we want to measure. As well, there may be interactions between these predictors that we will be unable to view using SLR. MLR allows us to both model and view the amalgamation of these predictors in their effects on the response variable. MLR from both an efficiency and relevancy perspective is much better suited to model your data as opposed to other methods. 

## Section 4. References

## The Data
```{r glimpse}
glimpse(apps)
```