---
title: "Understanding Google Play Application Reviews"
subtitle: "Project Proposal"
author: "RTime2Shine"
date: "October 29, 2019"
output: github_document
---

## Section 1. Introduction

As technology has become increasingly prevalent around the world, there has been a change in the consumption of media. One of these ways is via the purchase of applications (apps) for various smartphones and other devices. Several technology companies, including Apple and Google, run virtual stores for these apps in which a person can download an app for their device. These apps can be for various purposes like socializing, playing games, or watching television and movies, among others. While any user of a phone can agree that apps hold an important effect on how one interacts with technology on a daily basis, the weight of the impact becomes even more shocking when one looks at the figures- in 2018, global app downloads topped 194 billion (Dignan).

Our motivation for this project is to understand what makes apps (from the Google Play store specifically) have favorable ratings. Understanding the ratings of an app is important for several reasons. First, ratings can be important to the provider- in this case Google- who can decide whether an app should continue to be sold to maintain their quality standards. Ratings are additionally useful as a direct line of communication between the user and the developers- often, developers are made aware of changes that need to be made to their apps through user feedback. Lastly, reviews serve to inform potential users of an app whether or not it is worth their time and can affect future downloads. Considering that app users are predicted to spend about $120 billion in app stores in 2019, understanding which apps do well on the Play Store and what factors affect app performance is an immensely important question to gain more insight into. 

This leads us to introduce our main research questions- What are the relevant factors that affect the rating given for apps in the Google Play store? Although this project will give a detailed attempt to answer this question, our preliminary hypothesis is that the variables Category, Price/Type, Genre, Content Rating are the predictor variables that will most affect a given app rating and popularity, as measured by the number of installs of the app. Furthermore, once we test our hypothesis and determine which factors are relevant, we will attempt to use that information to predict the success of an app.

## Section 2. Exploratory Data Analysis

The dataset was obtained from Kaggle. According to Kaggle, the dataset was scraped directly from the Google Play Store in August 2018. Each observation represents one individual app on the Google Play Store. 

```{r load-packages, message=FALSE}
library(tidyverse)
library(broom)
library(knitr) 
library(skimr)
library(readr)
library(ggplot2)
library(stringr)
```

```{r load-data, message=FALSE}
apps <- read_csv("/cloud/project/02-data/googleplaystore.csv")
apps <- apps %>%
  na.omit(apps)
apps$Price <- str_replace(apps$Price, "\\$", "")
apps$Size <- str_replace(apps$Size, "\\M", "")
apps$Installs <- str_replace(apps$Installs, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "\\+", "")
apps$`Content Rating`<- str_replace(apps$`Content Rating`, "[[:digit:]]+", "")
```

```{r rating-distribution}
ggplot(data = apps, aes(x = Rating)) + geom_histogram(binwidth = 0.1, fill = "blue") + xlim(0,5) + labs(title = "Distribution of App Ratings")
```

```{r summary-stats}
apps %>% 
  summarise(median(Rating), IQR(Rating))
```

```{r count}
count(apps, Category) %>% 
  arrange(desc(n)) 
```

As shown above, qe determined median and IQR as our summary statistics because the distribution of `rating` appears to be slightly left-skewed. The median rating of an app is approximately **4.3** and the IQR is **0.5**.


To conduct a bivariate analysis, we will be making a pairs plot. 
```{r pairs-plot}
#pairs(Rating ~  Reviews + Size + Price + `Content Rating` + Genres, data = apps)
```

Given the above pairs plot, we will be investigating some relationships more in depth: 
```{r category-rating}
ggplot(data = apps, aes(x = Category, y = Rating)) + geom_boxplot() + coord_flip() +
  labs(title = "Relationship between Category and Rating") 
```

Although there is some variation in rating between app categories, the most telling aspect of this exploratory model is the outliers. It appears that some categories are more suspectible to outliers with low ratings. More over there are notable discrepancies between minimum boxplot rating among categories. 

```{r reviews-rating}
ggplot(data = apps, aes(x = Reviews, y = Rating)) + geom_point() +
  labs(title = "Relationship between Reviews and Rating", x = "Number of Reviews") 
```

Based on the scatterplot above, there is likely **not** a relationship between number of reviews and app rating. As the number of reviews increased the app rating was concentrated at approximately 4.5 - which was consistent with apps holding smaller number of reviews. 

```{r installs-rating}
ggplot(data = apps, aes(x = Installs, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Installs and Rating", x = "Number of Installs") + coord_flip()
```

The boxplot above clearly shows a significant relationship between number of installs and rating. As the number of installs increases the IQR appears to decrease in conjunction. Moreover median rating also increases with number of installs. 

```{r type-rating}
ggplot(data = apps, aes(x = Type, y = Rating)) + geom_boxplot() +
  labs(title = "Relationship between Type and Rating")
```

The boxplots for free and paid apps sport nearly identical median and IQR values. This tells us that whether an app is free or paid doesn't appear to have a major impact on the rating. Further analysis into the variation of rating among apps of different price levels is needed. 

## Section 3. Regression Analysis Plan

### Possible Interactions

```{r mutation}
apps$Price <- as.numeric(gsub('[$.]', '', apps$Price))
apps <- apps %>%
  mutate(Price = case_when(
  Price == 0 ~ "Free", 
  Price < 5 & Price > 0~ "Between $0 and $4.99", 
  Price > 5 ~ "Greater than $5"))

skim(apps, Price)
```

Since our question of interest is measuring the effect of various qualities of an app on its rating, there are a number of interactions within our predictor variables to consider. First, there is a possible interaction between content rating and categories. 

```{r int-content}
ggplot(apps, aes(x = Category, y = Rating, color = `Content Rating`)) + geom_point() + 
coord_flip() +
labs( title = "Relationship between Category and Rating", x ="Category", y = "Rating out of 5")
```

As shown in the plot above, there may be a correlation between having a lower content rating and being in a “family-friendly” category such as Family or game -  a clear example of this phenomenon is in the category dating. This interaction will have to be considered when building the model. As well, there is a clear interaction between other categories such as Mature or Teen being heavily represented among certain Categories. Secondly, there may be an interaction between number of reviews and rating. 

```{r int-reviews}
ggplot(apps, aes(x = Reviews, y = Rating, color = Reviews)) + 
  geom_point() +
  labs( title = "Relationship between Reviews and Rating", x ="# of Reviews ", y = "Rating out of 5")
```

As shown in this plot, as the number of reviews for an app increases, so does the rating generally. This is indicative of an app being popular so as there are more reviews there is most likely more polarization in the ratings. There is most likely some interaction between these two variables in the dataset. Thirdly, there may be an interaction between Type and Price. Since Type is an indicator measuring wether an app is paid or free, all apps that are free will be correlated with apps that have a price = 0 and apps that are paid will be correlated with apps that have a price greater than 0. 

```{r int-type}
ggplot(apps, aes(x = Type, y = Rating, color = Price)) + 
  geom_point() +
  labs( title = "Relationship between Type & Rating", x ="Type", y = "Rating out of 5")
```

This is further illustrated through the above plot, which clearly shows this interaction. These interactions along with any further ones we may find after our preliminary analysis will have to be explored further and considered when building our model. 

### Model Selection

Our ultimate goal is to create the model which most accurately and concisely predicts the Rating of an app given the predictors in the dataset. We will attempt to choose a model using a minimization of BIC as our criteria as this will allow us to calculate a precise prediction of our response variable while also removing extraneous predictors. We will use BIC as the selection criteria as it penalizes more for erroneous predictors. We will not use R-squared as a criteria for model selection. R squared increases strictly as the number of predictors increases and does not tell us if these additional predictors are significant or not. If we used r-squared we would always choose models with the largest numbers of predictors, which would not always produce the simplest, most accurate model. Unlike R squared, AIC, BIC, and adjusted R squared do penalize for insignificant predictors and can give us a better idea of which predictors actually contribute to the response variable. 

In order to find our final model, we will use a process of backwards selection slowly adding a combination of relevant predictors into our model. We will then check the BIC values for each of these models as well as the adj-R squared values and find the model with the highest value- this will be the model that most accurately predicts our response with the fewest number of predictors. We will then plot each predictor on the response to determine if the effect is relevant or if there are possible interactions between other variables. As well, we will need to consider potential outliers and extraneous values in our model. Using the distributions of standardized residuals and a calculation of Cook’s distance, we will attempt to determine those observations with high standardized residuals or cook’s distance and determine if those observations have a significant effect on our model. Lastly, we will need to find the VIF factor for each of our final predictors to see if there is any collinearity between them. A VIF greater than 10 would require us to explore possible ways to mitigate interactions between variables or consider dropping predictors are are too heavily correlated. 

### Use of Model and Reason

The regression modeling technique we will use will be Multiple Linear Regression (MLR). Since we are exploring the effect of multiple predictor variables on our response, `rating`, it is apt that we use MLR to model our data. MLR allows us to see the effect of multiple predictors on a response and explore both the significance of each predictor on the response as well as the effect of each predictor on the response. As opposed to Simple Linear Regression, MLR allows us to measure the effect of multiple predictors on your response in one model - SLR only allows us to measure the effect of one predictor on the response in one model. This is very taxing and inefficient for the number of predictors we want to measure. As well, there may be interactions between these predictors that we will be unable to view using SLR. MLR allows us to both model and view the amalgamation of these predictors in their effects on the response variable. MLR from both an efficiency and relevancy perspective is much better suited to model your data as opposed to other methods. 

## Section 4. References

Gupta, Lavanya. Kaggle. Jan. 2019, www.kaggle.com/lava18/google-play-store-apps?fbclid=IwAR36EMS2jg5fhPi-BQlX6Mv4MCk8YUm2XmyOLt0zsKkNyc9JK-JD7aLy-6I. Accessed 30 Oct. 2019. 

Dignan, Larry. “App Economy Expected to Be $120 Billion in 2019 as Small Screen Leads Digital Transformation Efforts.” ZDNet, ZDNet, 16 Jan. 2019, www.zdnet.com/article/app-economy-expected-to-be-120-billion-in-2019-as-small-screen-leads-digital-transformation-efforts/

## The Data

```{r glimpse, message=FALSE}
glimpse(apps)
```